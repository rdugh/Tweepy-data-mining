{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SDGs.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "N3ZLvFrM7rRw",
        "5Fs-Hfvo-Y8b",
        "jQybSTqP9ANi",
        "KGGwEh7xcIsw"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.3 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3ZLvFrM7rRw"
      },
      "source": [
        "# import Data\n",
        "## import and install important libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o43KwlBJ6UYp"
      },
      "source": [
        "from sklearn import preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhN_GSxKmKrp"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "from os import listdir\n",
        "from os.path import join\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9KucAcklVM1"
      },
      "source": [
        "'''!pip uninstall nltk\n",
        "!pip install nltk==3.2.5\n",
        "import nltk'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kVfAU1m387X"
      },
      "source": [
        "import emoji#checking if a character is an emoji\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import word_tokenize \n",
        "from nltk.corpus import stopwords \n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8T9AiWuaql3"
      },
      "source": [
        "'''folder=f'/Users/livi/Documents/2020 Fall/data mining/Proposal/Final Paper/Tweets_Half_Cleaned/'\n",
        "FinFolder=os.listdir(folder)\n",
        "Frame=[]\n",
        "for file in FinFolder[:]:\n",
        "    if (file.endswith('.csv')) & (file!='.DS_Store'):\n",
        "        One_Frame=pd.read_csv(join(folder,file),lineterminator='\\n',index_col=0)\n",
        "        Frame.append(One_Frame)\n",
        "T=pd.concat(Frame)\n",
        "T.reset_index(inplace=True,drop=True)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#T=pd.read_csv('/Users/livi/Git/Tweepy-data-mining/T12292020.csv', index_col=0,lineterminator='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "T=pd.read_csv('/Users/livi/Git/Tweepy-data-mining/LIWC2015 Results (T12292020).csv', index_col=0,lineterminator='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#rename columnsT\n",
        "T_cols=list(T.columns)\n",
        "T_cols[:18]=['created_at', 'source', 'extended_tweet', 'location', 'CountryCode','SDG', 'id', 'name', 'screen_name', 'url', 'friends_count','followers_count', 'hashtags', 'extended_tweet_lemmatized', 'neg','neu', 'pos', 'compound']\n",
        "T.columns=T_cols\n",
        "\n",
        "# LIWC columns\n",
        "AffectWords=['affect','posemo', 'negemo', 'anx', 'anger', 'sad']\n",
        "CognetiveProcess=['cogproc','insight', 'cause', 'discrep', 'tentat', 'certain', 'differ']\n",
        "PerpetualProcesses=['percept','see', 'hear', 'feel']\n",
        "BiologicalProcesses=['bio', 'body', 'health', 'sexual', 'ingest']\n",
        "CoreDrivesandNeeds=['drives','affiliation', 'achieve', 'power', 'reward', 'risk']\n",
        "TimeOrientation=['focuspast', 'focuspresent', 'focusfuture']\n",
        "Relativity=['relativ','motion', 'space', 'time']\n",
        "PersonalConcerns=['work', 'leisure', 'home', 'money', 'relig', 'death']\n",
        "InformalSpeech=['informal','swear','netspeak','assent','nonflu','filler']\n",
        "PersonalPronouns=['ppron','i','we','you','shehe','they']\n",
        "SummaryVariable=['Analytic','Clout','Authentic','Tone']\n",
        "Customized=['neg','pos','compound']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#drop ['location','name','url']\n",
        "desired_col=['created_at', 'source', 'extended_tweet', 'CountryCode','SDG', 'id', 'screen_name','followers_count', 'hashtags', 'extended_tweet_lemmatized','neu', 'neg','pos', 'compound']\n",
        "for i in ['AffectWords','CognetiveProcess','PerpetualProcesses','BiologicalProcesses','CoreDrivesandNeeds','TimeOrientation','Relativity','PersonalConcerns','InformalSpeech','PersonalPronouns','SummaryVariable']:\n",
        "  desired_col.extend(eval(i))\n",
        "T=T[desired_col]\n",
        "T.drop(index=T[pd.isnull(T['extended_tweet'])].index,inplace=True)\n",
        "T.drop(index=T[pd.isnull(T['id'])].index,inplace=True)\n",
        "T.reset_index(drop=True,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Country code clean\n",
        "for i,k in enumerate(T['CountryCode']):\n",
        "    if ']' in str(k):\n",
        "        T.loc[i,'CountryCode']='nan'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9xbxqE86jXe"
      },
      "source": [
        "#str\n",
        "for col in ['source','extended_tweet','CountryCode','id','extended_tweet_lemmatized']:\n",
        "    T[col]=T[col].astype(str)\n",
        "'''#num\n",
        "for col in ['friends_count','followers_count','neg','neu','pos','compound']:\n",
        "    T[col]=T[col].astype(float)'''\n",
        "#apply\n",
        "for col in ['SDG','hashtags']:\n",
        "    T[col]=T[col].apply(eval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "T['extended_tweet']=T['extended_tweet'].str.replace('\\n',' ')\n",
        "T['extended_tweet']=T['extended_tweet'].str.replace('#','')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in T.columns:\n",
        "    if T[i].dtypes=='float64':\n",
        "        T[i].replace(0,np.nan,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for n,i in enumerate(T['SDG']):\n",
        "    if isinstance(i,float):\n",
        "        T.drop(index=n,inplace=True)\n",
        "T.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "T.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "T['followers_count'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SFaRepraql4"
      },
      "source": [
        "# Data Cleaning\n",
        "\n",
        "## Generate user information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzGCtPBtaql5"
      },
      "source": [
        "### Detect Bot and Consider label Bot\n",
        "1. interarrival time is smaller than 5 seconds(Tested, mainly small intervals are caused by long tweets)\n",
        "2. other weird resource (source from bot may have interarrival time much longer 5 sec) Most effective method\n",
        "3. number of tweets per day (statuses_count, not used here, because there are accounts which frequently tweets)\n",
        "\n",
        "number of Influencer:A micro-influencer is someone who has between 1,000 to 100,000 followers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6UmU_dl6jXf"
      },
      "source": [
        "#Assign user type\n",
        "T['UserType']=''\n",
        "screen_name_of_bots=list(T[T['source'].str.contains('bot')]['screen_name'].unique())\n",
        "screen_name_of_bots.extend(['trendsinAI','form_bot','UDHROne257_247','globalhealthbot'])\n",
        "screen_name_of_bots.extend(list(T[T['screen_name'].str.contains('retweet',case=False)]['screen_name'].unique()))\n",
        "BFF=T[['screen_name','followers_count']].groupby(['screen_name']).agg({'followers_count':'max'}).reset_index()\n",
        "#assign user\n",
        "screen_name_of_user=list(BFF[(BFF['followers_count']<=800)]['screen_name'].unique())\n",
        "T.loc[(T['screen_name'].isin(screen_name_of_user))& (~ T['screen_name'].isin(screen_name_of_bots)),'UserType']='user'\n",
        "#assign influencer\n",
        "screen_name_of_influencer=list(BFF[(BFF['followers_count']>800)]['screen_name'].unique())\n",
        "screen_name_of_influencer.extend(list(T[T['screen_name'].str.contains('news',case=False)]['screen_name'].unique()))\n",
        "T.loc[(T['screen_name'].isin(screen_name_of_influencer)) & (~ T['screen_name'].isin(screen_name_of_bots)),'UserType']='influencer'\n",
        "#assign bots\n",
        "T.loc[T['screen_name'].isin(screen_name_of_bots),'UserType']='bot'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "T=T[T['UserType']=='user'].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "T.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSAy5lHG6jXg"
      },
      "source": [
        "### include other SDG according to the keywords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "JfuWuF096jXg"
      },
      "source": [
        "'''#choose to include keywords for sdgs or not\n",
        "def addsdg(df):\n",
        "    newdf=df.copy(deep=True)\n",
        "    keyword_list=[['#poverty'],['#zerohunger'],['#globalhealth'],['#education'],['#genderequality'],['#water'],['#energy'],['#decentwork'],['#economicgrowth','#ideas'],    ['#socialjustice'],['#sustainablecities'],['#sparetosave'],['#climateaction'],['#ocean'],['#lifeonland'],['#justice','#peace']]\n",
        "    for i in range(16):\n",
        "        for ii in keyword_list[i]:\n",
        "            for iii in newdf[newdf['hashtags'].apply(lambda x: ii in x)].index:\n",
        "                newdf.loc[iii,'SDG'].append('SDG'+str(i+1))\n",
        "            #print(i+1,keyword_list[i],ii)\n",
        "    \n",
        "    return newdf'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fs-Hfvo-Y8b"
      },
      "source": [
        "###  Generate labels topic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cviRf6un-U0Z"
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJM3mPlW6tcU"
      },
      "source": [
        "'''PopSDGs=['SDG'+str(i) for i in [4,5,7,2,16]]\n",
        "for PopSDG in PopSDGs:\n",
        "  searchTerm = PopSDG\n",
        "  TextExt=T[[searchTerm in x for x in T['SDG']]]\n",
        "  #parameter set-up\n",
        "  count_vect=CountVectorizer(max_df=0.8,min_df=2,stop_words='english')\n",
        "  LDA = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "  #topic modeling\n",
        "  tweet_matrix=count_vect.fit_transform(TextExt['extended_tweet_lemmatized'].values.astype('U'))\n",
        "  LDA.fit(tweet_matrix)\n",
        "  #print results\n",
        "   for i in top_topic_words:\n",
        "      print(count_vect.get_feature_names()[i])\n",
        "  for i,topic in enumerate(LDA.components_):\n",
        "      print(f'Top 20 words for {PopSDG} topic #{i}:')\n",
        "      print([count_vect.get_feature_names()[i] for i in topic.argsort()[-20:]])\n",
        "      print('\\n')\n",
        "  print('*'*50)\n",
        "  #put topic back to tweets\n",
        "  topic_values = LDA.transform(tweet_matrix)\n",
        "  T[PopSDG+'Topic']=None\n",
        "  #putinto the main matrix\n",
        "  T.loc[TextExt.index,PopSDG+'Topic']=topic_values.argmax(axis=1)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "### Select Countries"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selected_country=['United States','United Kingdom','Australia','India','Nigeria']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### Developing and Developed Country Compare (vote the top then select)\n",
        "#define developed country and developing country\n",
        "developed_country='Canada,United States,Australia,Japan,New Zealand,Austria,Belgium,Denmark,Finland,France,Germany,Greece,Ireland,Italy,Luxembourg,Netherlands,Portugal,Spain,Sweden,United Kingdom,Bulgaria,Croatia,Cyprus,Czechia,Estonia,Hungary,Latvia,Lithuania,Malta,Poland,Romania,Slovakia,Slovenia,Iceland,Norway,Switzerland'.split(',')\n",
        "transition='Albania,Bosnia and Herzegovina,Montenegro,North Macedonia,Serbia,Armenia,Azerbaijan,Belarus,Georgia,Kazakhstan,Kyrgyzstan,Moldova,Russia,Tajikistan,Turkmenistan,Ukraine,Uzbekistan'.split(',')\n",
        "developing_country='Algeria,Egypt,Libya,Mauritania,Morocco,Sudan,Tunisia,Cameroon,Central African Republic,Chad,Republic of the Congo,Equatorial Guinea,Gabon,São Tomé and Príncipe,Burundi,Comoros,Democratic Republic of the Congo,Djibouti,Eritrea,Ethiopia,Kenya,Madagascar,Rwanda,Somalia,South Sudan,Uganda,Tanzania,Angola,Botswana,Eswatini,Lesotho,Malawi,Mauritius,Mozambique,Namibia,South Africa,Zambia,Zimbabwe,Benin,Burkina Faso,Cape Verde,Côte d\\'Ivoire,The Gambia,Ghana,Guinea,Guinea-Bissau,Liberia,Mali,Niger,Nigeria,Senegal,Sierra Leone,Togo,Brunei,Cambodia,China,North Korea,Fiji,Hong Kong,Indonesia,Kiribati,Laos,Malaysia,Mongolia,Myanmar (Burma),Papua New Guinea,Philippines,South Korea,Samoa,Singapore,Solomon Islands,Taiwan,Thailand,Timor-Leste,Vanuatu,Vietnam,Afghanistan,Bangladesh,Bhutan,India,Iran,Maldives,Nepal,Pakistan,Sri Lanka,Bahrain,Iraq,Israel,Jordan,Kuwait,Lebanon,Oman,Qatar,Saudi Arabia,Palestine,Syria,Turkey,United Arab Emirates,Yemen,The Bahamas,Barbados,Belize,Guyana,Jamaica,Suriname,Trinidad and Tobago,Costa Rica,Cuba,Dominican Republic,El Salvador,Guatemala,Haiti,Honduras,Mexico,Nicaragua,Panama,Argentina,Bolivia,Brazil,Chile,Colombia,Ecuador,Paraguay,Peru,Uruguay,Venezuela'.split(',')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jh6Os6A19y-"
      },
      "source": [
        "# Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNFhFfuM6jXh"
      },
      "source": [
        "Users=T[['screen_name','id','UserType','source']].groupby('screen_name').agg({'id':'count','UserType':'last','source':'last'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMjklrFT6jXh"
      },
      "source": [
        "Users.sort_values('id',ascending=False,inplace=True)\n",
        "Users.reset_index(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfJs9CL36jXh"
      },
      "source": [
        "cri_bot=Users['UserType']=='bot'\n",
        "cri_user=Users['UserType']=='user'\n",
        "cri_influencer=Users['UserType']=='influencer'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIaw28JH6jXi"
      },
      "source": [
        "set(Users['screen_name'].to_list()[:30]).intersection(set(Users[cri_user]['screen_name'].to_list()[:15]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XmZdxZv6jXi"
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.bar(Users['screen_name'].to_list()[:40],Users['id'].to_list()[:40])\n",
        "plt.title('All', fontsize=15)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.bar(Users[cri_bot]['screen_name'].to_list()[:15],Users[cri_bot]['id'].to_list()[:15])\n",
        "plt.title('Bot', fontsize=15)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.bar(Users[cri_user]['screen_name'].to_list()[:30],Users[cri_user]['id'].to_list()[:30])\n",
        "plt.title('User', fontsize=15)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.bar(Users[cri_influencer]['screen_name'].to_list()[:15],Users[cri_influencer]['id'].to_list()[:15])\n",
        "plt.title('Influencer', fontsize=15)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgMrYeEs6jXi"
      },
      "source": [
        "T[['screen_name','id','UserType','source']].groupby('UserType').agg({'id':'count','source':'last','screen_name':'last'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Frequency of SDG"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TT=T[['extended_tweet', 'SDG','CountryCode']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TT['length']=TT['SDG'].apply(lambda x: type(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''for n,i in enumerate(TT['SDG']):\n",
        "    if isinstance(i,float):\n",
        "        TT.drop(index=n,inplace=True)\n",
        "        T.drop(index=n,inplace=True)\n",
        "TT.reset_index(drop=True, inplace=True)\n",
        "T.reset_index(drop=True, inplace=True)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TT['length']=TT['SDG'].apply(lambda x: len(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TT=TT[[len(i)>0 for i in TT['SDG']]]\n",
        "TT.reset_index(drop=True,inplace=True)\n",
        "print(TT.shape)\n",
        "TT.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Clean up the string\n",
        "Tdict={}\n",
        "for i in TT['SDG'].to_list():\n",
        "    for ii in i:\n",
        "        if ii in Tdict:\n",
        "            Tdict[ii]+=1\n",
        "        else:\n",
        "            Tdict[ii]=1\n",
        "#Tdict['']+=Tdict['']\n",
        "try:\n",
        "  Tdict['SDG1']+=Tdict['SDG01']\n",
        "  Tdict['SDG8']+=Tdict['SDG08']\n",
        "  Tdict['SDG6']+=Tdict['SDG64']\n",
        "  Tdict['SDG4']+=Tdict['SDG04']\n",
        "  del Tdict['SDG01']\n",
        "  del Tdict['SDG04']\n",
        "  #del Tdict['SDG4ALL']\n",
        "  #del Tdict['SDG4B']\n",
        "  #del Tdict['SDG4IT']\n",
        "  #del Tdict['SDG4SURVEY']\n",
        "  #del Tdict['SDG4PT7']\n",
        "  del Tdict['SDG64']\n",
        "  del Tdict['SDG08']\n",
        "  del Tdict['SDG18']\n",
        "  del Tdict['SDG200']\n",
        "  del Tdict['SDG2030']\n",
        "  del Tdict['SDG2020']\n",
        "  del Tdict['SDG19']\n",
        "\n",
        "except:\n",
        "  #raise\n",
        "  pass\n",
        "Tdict={k:v for k,v in sorted(Tdict.items(), key=lambda x: x[1],reverse=True)}\n",
        "sorted(Tdict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(Tdict.keys())\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.bar(Tdict.keys(),Tdict.values())\n",
        "plt.xlabel('SDG',fontsize=25,weight='bold')\n",
        "plt.xticks(fontsize=20,weight='bold',rotation=90)\n",
        "plt.yticks(fontsize=20,weight='bold',rotation=90)\n",
        "plt.ylabel('Frequency',fontsize=25,weight='bold')\n",
        "plt.tight_layout()\n",
        "#plt.savefig('SDG popularity.jpg',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQybSTqP9ANi"
      },
      "source": [
        "## Generate National Graph\n",
        "### import libraty and geojson"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zo01q7vrhVr1"
      },
      "source": [
        "import folium"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6LmhSwJhgGn"
      },
      "source": [
        "with open('/Users/livi/Git/Tweepy-data-mining/countries.geojson') as f:\n",
        "  geodata = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TWIyC4sjRzU"
      },
      "source": [
        "holdlist=[]\n",
        "allcountries=[]\n",
        "for k,i in enumerate(geodata['features']):\n",
        "  allcountries.append(i['properties']['ADMIN'])\n",
        "  if i['properties']['ADMIN'] in T['CountryCode'].tolist():\n",
        "    holdlist.append(i['properties']['ADMIN'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx8-Ft05uzTT"
      },
      "source": [
        "print('Overlap of geojson and countrycode:',len(np.unique(np.array(holdlist))))\n",
        "print('CountryCode in data:',len(T['CountryCode'].unique()))\n",
        "print('Geojson we have:',len(np.unique(allcountries)))\n",
        "df1=pd.DataFrame(allcountries)\n",
        "df1.index=df1[0]\n",
        "df1.columns=['geojson']\n",
        "df2=pd.DataFrame(T['CountryCode'].unique())\n",
        "df2.index=df2[0]\n",
        "df2.columns=['googlemap']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDY4EPyV6jXk"
      },
      "source": [
        "#countries could not be identified\n",
        "countrynotincluded=list(set(T['CountryCode'].unique())-set(np.unique(np.array(holdlist))))\n",
        "sum(T['CountryCode'].isin(countrynotincluded)),len(T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXGl0JF3w8Y8"
      },
      "source": [
        "'''contriesLists=pd.concat([df1, df2], axis=1, sort=False,join='outer')\n",
        "D=contriesLists[contriesLists['googlemap']!=contriesLists['geojson']]'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHWAzABZlRhV"
      },
      "source": [
        "### Plot Frequency Map and sccatter plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PP=T[['CountryCode', 'id']].groupby('CountryCode').count()\n",
        "PP.reset_index(inplace=True)\n",
        "PP.sort_values('id',ascending=False,inplace=True)\n",
        "PP['count']=np.log10(PP['id'])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBpnccR4inwu"
      },
      "source": [
        "#frequency map of tweets\n",
        "'''m = folium.Map(location=[0, 0], zoom_start=2)\n",
        "folium.Choropleth(\n",
        "    geo_data=geodata,\n",
        "    name='choropleth',\n",
        "    data=PP,\n",
        "    columns=['CountryCode', 'count'],\n",
        "    key_on='feature.properties.ADMIN',\n",
        "    fill_color='YlGn',\n",
        "    fill_opacity=0.7,\n",
        "    line_opacity=0.2,\n",
        "    legend_name='# of Tweets'\n",
        ").add_to(m)\n",
        "\n",
        "folium.LayerControl().add_to(m)\n",
        "m'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDKGMBW2GFRK"
      },
      "source": [
        "#plot of all countries and counts\n",
        "'''plt.figure(figsize=(20,10))\n",
        "#plt.bar(PP.CountryCode[:30],PP.text[:30])\n",
        "plt.scatter(range(len(PP.CountryCode)),PP['count'])\n",
        "plt.xlabel('Country',fontsize=20)\n",
        "plt.xticks(rotation=90,fontsize=15)\n",
        "plt.yticks(fontsize=15)\n",
        "plt.ylabel('log_10 Frequency',fontsize=20)\n",
        "plt.tight_layout()\n",
        "#plt.savefig('1023place dis.jpg')\n",
        "plt.show()'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhaFNIWU6jXl"
      },
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "plt.bar(PP.CountryCode[1:30],PP.id[1:30])\n",
        "plt.xlabel('Country',fontsize=20)\n",
        "plt.xticks(rotation=90,fontsize=15)\n",
        "plt.yticks(fontsize=15)\n",
        "plt.ylabel('Frequency',fontsize=20)\n",
        "plt.tight_layout()\n",
        "#plt.savefig('Tweets Frequency with countries.jpg',dpi=300)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khGaoa3naVH9"
      },
      "source": [
        "### plot SDG Map and heatmap table \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_UIUjUecggz"
      },
      "source": [
        "#Country and SDG distribution\n",
        "CS=T[['CountryCode','SDG']]\n",
        "SDs={}\n",
        "for coun in CS['CountryCode'].unique():\n",
        "    SDs[coun]={'SDG'+str(i+1):0 for i in range(17)}\n",
        "    temp=CS[CS['CountryCode']==coun]['SDG']\n",
        "    for S in temp:\n",
        "        for SS in S:\n",
        "            try:\n",
        "              SDs[coun][SS]+=1\n",
        "            except:\n",
        "                pass  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#### Top SDG based on Country"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''top5EnCoun=PP.CountryCode[1:4].tolist()\n",
        "for topcount in top5EnCoun:\n",
        "  plt.figure(figsize=(14,7))\n",
        "  plt.bar(SDs[topcount].keys(),SDs[topcount].values())\n",
        "  plt.title(topcount,fontsize=25,weight='bold')\n",
        "  plt.xlabel('SDG',fontsize=25,weight='bold')\n",
        "  plt.xticks(fontsize=20,weight='bold',rotation=90)\n",
        "  plt.yticks(fontsize=20,weight='bold',rotation=90)\n",
        "  plt.ylabel('Frequency',fontsize=25,weight='bold')\n",
        "  plt.tight_layout()\n",
        "  #plt.savefig(topcount+'.jpg')'''"
      ]
    },
    {
      "source": [
        "#### distribution heatmap "
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnQq5FUU0g3m"
      },
      "source": [
        "SD_Coun_HM=pd.DataFrame({i:SDs[i] for i in selected_country})\n",
        "rowname,colname=SD_Coun_HM.columns,SD_Coun_HM.index\n",
        "#prepare label\n",
        "HM_label=SD_Coun_HM.div(SD_Coun_HM.sum(axis=0),axis=1).T.values\n",
        "#prepare data\n",
        "SD_Coun_HM=pd.DataFrame(preprocessing.MinMaxScaler().fit_transform(SD_Coun_HM).T)\n",
        "SD_Coun_HM.index,SD_Coun_HM.columns=rowname,colname\n",
        "#plt\n",
        "plt.figure(figsize=(20,4))\n",
        "axhm = plt.gca()\n",
        "axhm.xaxis.set_ticks_position('top')\n",
        "cmp=sns.color_palette(\"Blues\", as_cmap=True)\n",
        "sns.heatmap(SD_Coun_HM,cmap=cmp,vmax=1.2,linewidths=1,annot=HM_label,fmt='.1%',cbar=False,annot_kws={\"fontsize\":13})\n",
        "plt.xticks(fontsize=15)\n",
        "plt.yticks(fontsize=15)\n",
        "#plt.savefig('HeatMap Country User.jpg',dpi=300)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#### SDG change over time"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''def SDGovertime(T):\n",
        "    for ii in range(4):\n",
        "        l=[]\n",
        "        plt.figure(figsize=(18,5))\n",
        "        for i in range(ii*5,(ii+1)*5,1):\n",
        "            searchterm='SDG'+str(i+1)\n",
        "            Trend=pd.DataFrame()\n",
        "            Trend['created_at']=T[T['SDG'].apply(lambda x: searchterm in x)]['created_at'].sort_values()\n",
        "            Trend['count']=1\n",
        "            Trend['created_at']=pd.to_datetime(Trend['created_at'])\n",
        "            plt.plot(Trend.groupby(pd.Grouper(key=\"created_at\", freq=\"1w\")).sum())\n",
        "            plt.xticks(rotation=90)\n",
        "            l.append(searchterm)\n",
        "        plt.legend(l,loc=2)\n",
        "    plt.show()\n",
        "\n",
        "SDGovertime(T[T['CountryCode']=='United States'])'''"
      ]
    },
    {
      "source": [
        "#### Distribution of SDG"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUvZ9XuC6jXl"
      },
      "source": [
        "def multi_dict_max(dict):\n",
        "    max_value=max(dict.values())\n",
        "    return [i for i in dict.keys() if dict[i]==max_value]\n",
        "def multi_dict_min(dict):\n",
        "    min_value=min(dict.values())\n",
        "    return [i for i in dict.keys() if dict[i]==min_value]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5dqzHNYayLK"
      },
      "source": [
        "#Color of each SDG\n",
        "Colors=['#e4253c','#dea73a','#4c9f45','#c5202e','#f0412b','#29bee2','#fac315','#a21c44','#f26a2c','#dd1768','#f99d27','#be8b2c','#417f45','#1c97d3','#5dbb47','#06699e','#18486b']\n",
        "SDGcolor={'SDG'+str(i+1):Colors[i] for i in range(17)}\n",
        "\n",
        "#statistic of some selected countries\n",
        "#statistic of some selected countries\n",
        "EsC_count={}\n",
        "for i in SDs.keys():\n",
        "    if i==i and i not in [\"['SDG14']\",\"[]\"]:\n",
        "        try:\n",
        "            EsC_count[i]={'sum':0,'top':0,'percentage':0}\n",
        "            EsC_count[i]['sum']=sum(SDs[i].values())\n",
        "            if EsC_count[i]['sum']!=0:\n",
        "                EsC_count[i]['percentage']=round(max(SDs[i].values())/sum(SDs[i].values())*100,1)\n",
        "            else:\n",
        "                EsC_count[i]['percentage']=0\n",
        "            EsC_count[i]['top']=multi_dict_max(SDs[i])\n",
        "            EsC_count[i]['bottom']=multi_dict_min(SDs[i])\n",
        "            #EsC_count[i]['geo']=[geodict[i]['lat'],geodict[i]['lng']]\n",
        "        except:\n",
        "            raise\n",
        "#select some countries\n",
        "ggdataEsC=geodata\n",
        "select=[]\n",
        "for i in range(len(ggdataEsC['features'])):\n",
        "    if ggdataEsC['features'][i]['properties']['ADMIN'] in EsC_count.keys():\n",
        "        select.append(i)\n",
        "ggdataEsC['features']=[ggdataEsC['features'][i] for i in select]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajCh_ZlPz0tT"
      },
      "source": [
        "#plot the graph least popular sdg\n",
        "def stfunc(features):\n",
        "    sort_features={i:Tdict[i] for i in EsC_count[features['properties']['ADMIN']]['bottom']}\n",
        "    sort_features=sorted(sort_features.items(),key=lambda x: x[1])\n",
        "    return {'fillOpacity': 0.9,'weight': 0,'fillColor':SDGcolor[sort_features[0][0]]}\n",
        "def gengeo(coun):\n",
        "    f={'features':[ggdataEsC['features'][i]] for i in range(len(ggdataEsC['features'])) if ggdataEsC['features'][i]['properties']['ADMIN']==coun}\n",
        "    f['type']='FeatureCollection'\n",
        "    return f\n",
        "\n",
        "o = folium.Map(location=[50, 10], zoom_start=2.2,tiles='OpenStreetMap')\n",
        "folium.GeoJson(ggdataEsC,name='United States',style_function=stfunc).add_to(o)\n",
        "o"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLIdB1U8a1-H"
      },
      "source": [
        "#plot the graph most popular\n",
        "def stfunc(features):\n",
        "    sort_features={i:Tdict[i] for i in EsC_count[features['properties']['ADMIN']]['top']}\n",
        "    sort_features=sorted(sort_features.items(),key=lambda x: x[1],reverse=True)\n",
        "    return {'fillOpacity': 0.9,'weight': 0,'fillColor':SDGcolor[sort_features[0][0]]}\n",
        "def gengeo(coun):\n",
        "    f={'features':[ggdataEsC['features'][i]] for i in range(len(ggdataEsC['features'])) if ggdataEsC['features'][i]['properties']['ADMIN']==coun}\n",
        "    f['type']='FeatureCollection'\n",
        "    return f\n",
        "\n",
        "n = folium.Map(location=[50, 10], zoom_start=2.2,tiles='OpenStreetMap')\n",
        "folium.GeoJson(ggdataEsC,name='United States',style_function=stfunc).add_to(n)\n",
        "n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_2xKYE1jNdZ"
      },
      "source": [
        "#### vote the top then select, developing and developed countries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAj1Q8CEj1Qt"
      },
      "source": [
        "def talkmost(country,T):\n",
        "  for i in list(set(country)-set(SDs.keys())):\n",
        "    country.remove(i)\n",
        "  country_SDs={i:SDs[i]for i in country}\n",
        "  country_TopSDG=[]\n",
        "  for i in country_SDs:\n",
        "    try:\n",
        "      if sum(country_SDs[i].values())>20 & max(country_SDs[i].values())>1:#make sure more than 1 tweets in the max column\n",
        "        MAXtop=max(country_SDs[i].values())\n",
        "        country_TopSDG.extend([ii for ii in country_SDs[i] if country_SDs[i][ii]==MAXtop])    \n",
        "    except:\n",
        "      pass\n",
        "  country_TopSDG={k:v for k,v in sorted(Counter(country_TopSDG).items(),key=lambda x:x[1],reverse=True)}\n",
        "  plt.bar(country_TopSDG.keys(),country_TopSDG.values())\n",
        "  plt.title(T)\n",
        "  print(list(country_TopSDG.keys()))\n",
        "  plt.show()\n",
        "\n",
        "def talkleast(country,T):\n",
        "  for i in list(set(country)-set(SDs.keys())):\n",
        "    country.remove(i)\n",
        "  country_SDs={i:SDs[i]for i in country}\n",
        "  country_TopSDG=[]\n",
        "  for i in country_SDs:\n",
        "    try:\n",
        "      if sum(country_SDs[i].values())>20:#make sure more than 1 tweets in the max column\n",
        "        MINtop=min(country_SDs[i].values())\n",
        "        country_TopSDG.extend([ii for ii in country_SDs[i] if country_SDs[i][ii]==MINtop])\n",
        "        #print(MINtop)\n",
        "    except:\n",
        "      pass\n",
        "  country_TopSDG={k:v for k,v in sorted(Counter(country_TopSDG).items(),key=lambda x:x[1],reverse=True)}\n",
        "  plt.bar(country_TopSDG.keys(),country_TopSDG.values())\n",
        "  plt.title(T)\n",
        "  print(list(country_TopSDG.keys()))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R145PSnG6jXn"
      },
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "talkmost(developed_country,'Developed talk Most')\n",
        "plt.figure(figsize=(15,5))\n",
        "talkleast(developed_country,'Developed talk Least')\n",
        "plt.figure(figsize=(15,5))\n",
        "talkmost(developing_country,'developing talk Most')\n",
        "plt.figure(figsize=(15,5))\n",
        "talkleast(developing_country,'developing talk Least')\n",
        "plt.figure(figsize=(15,5))\n",
        "talkmost(transition,'Transition talk Most')\n",
        "plt.figure(figsize=(15,5))\n",
        "talkleast(transition,'Transition talk Least')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrjJltzq_GB_"
      },
      "source": [
        "#### Based on Frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewv63om3AFaQ"
      },
      "source": [
        "def toptalkfreq(countrylist,T):\n",
        "  SD_Co=pd.DataFrame({i:SDs[i] for i in countrylist})\n",
        "  SD_Co.sum(axis=1).sort_values(ascending=False).plot(kind='bar')\n",
        "  l=list(SD_Co.sum(axis=1).sort_values(ascending=False).index)\n",
        "  #l = [i.strip('\\'\\'') for i in l]\n",
        "  print(l)\n",
        "\n",
        "  plt.title(T)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nzbc50w6jXn"
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "toptalkfreq(developed_country,'Developed Countries')\n",
        "plt.figure(figsize=(10,5))\n",
        "toptalkfreq(developing_country,'Developing Countries')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "### Print topic for SDG4,SDG2 and SDG5"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        " #group support to food security; end food waste; healthy\n",
        " for i in [4,2,5]:\n",
        "     exec('SDG'+str(i)+\"=PrintTopic(T[T['SDG'].apply(lambda x: 'SDG'+str(i) in x)],topicnum=3)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in [4,2,5]:\n",
        "    sdg_df=eval('SDG'+str(i))\n",
        "    for ii in range(3):\n",
        "        print('Topic ',ii, ' of ', 'SDG'+str(i))\n",
        "        print(*sdg_df[sdg_df['HateTopic']==ii]['extended_tweet'].sample(3).tolist(),sep='\\n'+'-'*50+'\\n')\n",
        "        print('\\n')\n",
        "    print('*'*50+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(*T[T['extended_tweet'].str.contains('sdg2 ') & T['extended_tweet'].str.contains('sdg13')]['extended_tweet'].sample(3).tolist(),sep='\\n'*3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SDG2_HT=[]\n",
        "for i in T[T['SDG'].apply(lambda x: 'SDG2' in x)]['hashtags']:\n",
        "    SDG2_HT.extend(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sorted(Counter(SDG2_HT).items(),key=lambda x:x[1],reverse=True)[:30]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGGwEh7xcIsw"
      },
      "source": [
        "## LIWC analysis"
      ]
    },
    {
      "source": [
        "#### meaning of columns analysis"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(T['compound']))\n",
        "print(sum(T['compound']>0)/len(T['compound']),sum(T['compound']<0)/len(T['compound']),sum(T['compound'].isnull())/len(T['compound']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import scipy.stats as stats\n",
        "from itertools import combinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''# LIWC columns\n",
        "AffectWords=['posemo', 'negemo', 'anx', 'anger', 'sad']\n",
        "CognetiveProcess=['insight', 'cause', 'discrep', 'tentat', 'certain', 'differ']\n",
        "PerpetualProcesses=['see', 'hear', 'feel']\n",
        "BiologicalProcesses=['bio', 'body', 'health', 'sexual', 'ingest']\n",
        "CoreDrivesandNeeds=['affiliation', 'achieve', 'power', 'reward', 'risk']\n",
        "TimeOrientation=['focuspast', 'focuspresent', 'focusfuture']\n",
        "Relativity=['motion', 'space', 'time']\n",
        "PersonalConcerns=['work', 'leisure', 'home', 'money', 'relig', 'death']\n",
        "InformalSpeech=['informal','swear','netspeak','assent','nonflu','filler']\n",
        "PersonalPronouns=['i','we','you','shehe','they']\n",
        "Customized=['neg','pos','compound']'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#hear and see represent the discussion of current situation, feel represent the tendency for a change"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "not_selected_direction=['PerpetualProcesses','BiologicalProcesses','CognetiveProcess','InformalSpeech','Relativity','PersonalPronouns']\n",
        "selected_direction=['AffectWords','Customized','CoreDrivesandNeeds','PersonalConcerns','TimeOrientation']\n",
        "other=['SummaryVariable','Customized']"
      ]
    },
    {
      "source": [
        "#### Study of Selected Country "
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMTRrikItZaj"
      },
      "source": [
        "T_COUN=T[T['CountryCode'].isin(selected_country)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#all of the data is not normally distributed\n",
        "'''for LIWCcol in ['PersonalPronouns','AffectWords','CognetiveProcess','PerpetualProcesses','BiologicalProcesses','CoreDrivesandNeeds','TimeOrientation','Relativity','PersonalConcerns','InformalSpeech','Customized','SummaryVariable']:\n",
        "    for i in eval(LIWCcol):\n",
        "        for coun in selected_country:\n",
        "            print(LIWCcol,'shapiro',coun,stats.shapiro(T_COUN[T_COUN['CountryCode']==coun][i]))'''\n",
        "#Basically everything is normally distributed except summary attributes\n",
        "\n",
        "#create a stats table\n",
        "OneFrame=[]\n",
        "for LIWCcol in selected_direction:\n",
        "    for i in eval(LIWCcol):\n",
        "        CounCom=combinations(selected_country,2)\n",
        "        for couns in CounCom:\n",
        "            st=stats.ttest_ind(T_COUN[T_COUN['CountryCode']==couns[0]][i].dropna(),T_COUN[T_COUN['CountryCode']==couns[1]][i].dropna(),equal_var=False)\n",
        "            OneRow=[LIWCcol,i,couns[0],couns[1],st[0],st[1]]\n",
        "            OneFrame.append(OneRow)\n",
        "            #print(i,':',couns,stats.ranksums(T_COUN[T_COUN['CountryCode']==couns[0]][i],T_COUN[T_COUN['CountryCode']==couns[1]][i])[1])\n",
        "stats_table=pd.DataFrame(OneFrame,columns=['keyword1','keyword2','coun1','coun2','test_stats','p-value'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#stats_table.to_csv('User stats table based on countries.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stats_table1=stats_table[stats_table['p-value']<0.05].sort_values('p-value')\n",
        "stats_table1['coun']=[frozenset([stats_table1.loc[i,'coun1'],stats_table1.loc[i,'coun2']]) for i in stats_table1.index]\n",
        "\n",
        "stats_table2=stats_table1.groupby('coun').agg({'keyword1':','.join,'keyword2':','.join})\n",
        "stats_table2['kw1list']=stats_table2['keyword1'].apply(lambda x: set(x.split(',')))\n",
        "stats_table2['kw2list']=stats_table2['keyword2'].apply(lambda x: set(x.split(',')))\n",
        "\n",
        "stats_table2['kw1len']=stats_table2['kw1list'].apply(lambda x:len(x))\n",
        "stats_table2['kw2len']=stats_table2['kw2list'].apply(lambda x:len(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in selected_direction:\n",
        "    display(stats_table1[(stats_table1['coun'].apply(lambda x: 'Nigeria' in x)) & (stats_table1['keyword1']==i)].sort_values('keyword2'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#choose to include keywords for sdgs or not\n",
        "def addsdg(df):\n",
        "    newdf=df.copy(deep=True)\n",
        "    keyword_list=[['#poverty'],['#zerohunger'],['#globalhealth'],['#education'],['#genderequality'],['#water'],['#energy'],['#decentwork'],['#economicgrowth','#ideas'],    ['#socialjustice'],['#sustainablecities'],['#sparetosave'],['#climateaction'],['#ocean'],['#lifeonland'],['#justice','#peace']]\n",
        "    for i in range(16):\n",
        "        for ii in keyword_list[i]:\n",
        "            for iii in newdf[newdf['hashtags'].apply(lambda x: ii in x)].index:\n",
        "                newdf.loc[iii,'SDG'].append('SDG'+str(i+1))\n",
        "            #print(i+1,keyword_list[i],ii)\n",
        "    return newdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ranksdg(df, add=False):\n",
        "    newdf=df\n",
        "    df_SDG=[]\n",
        "    for i in newdf['SDG'].to_list():\n",
        "        df_SDG.extend(i)\n",
        "    if add==True:\n",
        "        keyword_list=[['#poverty'],['#zerohunger'],['#globalhealth'],['#education'],['#genderequality'],['#water'],['#energy'],['#decentwork'],['#economicgrowth','#ideas'],    ['#socialjustice'],['#sustainablecities'],['#sparetosave'],['#climateaction'],['#ocean'],['#lifeonland'],['#justice','#peace']]\n",
        "        for i in range(16):\n",
        "            for ii in keyword_list[i]:\n",
        "                for iii in newdf[newdf['hashtags'].apply(lambda x: ii in x)].index:\n",
        "                    df_SDG.append('SDG'+str(i+1))\n",
        "    print(sorted(Counter(df_SDG).items(),key=lambda x:x[1], reverse=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#analysis of country\n",
        "T_happy_Indian=T[(T['CountryCode']=='India') & (T['compound'] >0.9)]\n",
        "T_sad_Australia=T[(T['CountryCode']=='Australia') & (T['compound'] <-0.5)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "T_Nigeria=T[(T['CountryCode']=='Nigeria')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "T_US=T[(T['CountryCode']=='United States')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(*T_US[T_US['you']>10]['extended_tweet'].sample(3).tolist(),sep='\\n'*3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#print(*T_sad_Australia[T_sad_Australia['extended_tweet'].str.contains('sdg13')]['extended_tweet'].drop_duplicates().sample(5).tolist(),sep='\\n'*3)\n",
        "'''ranksdg(T_happy_Indian,add=True)\n",
        "print('\\n')\n",
        "ranksdg(T_sad_Australia,add=True)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for LIWCcol in selected_direction:\n",
        "    colneeded=['CountryCode','id']\n",
        "    colneeded.extend(eval(LIWCcol))\n",
        "    aggdict={i:['mean','count'] for i in eval(LIWCcol)}\n",
        "    aggdict['id']='count'\n",
        "    DS=T_COUN[colneeded].groupby('CountryCode').agg(aggdict)\n",
        "    display(DS)\n",
        "    #DS.to_csv(LIWCcol+'median User.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for LIWCcol in selected_direction:\n",
        "    colneeded=[]\n",
        "    colneeded.extend(eval(LIWCcol))\n",
        "    for col in colneeded:\n",
        "        plt.figure(figsize=(10,10))\n",
        "        for cou in selected_country:\n",
        "            plt.hist(np.log(T_COUN[T_COUN['CountryCode']==cou][col]),alpha=0.7,bins=100)\n",
        "        plt.xlim(xmin=0, xmax = 5)\n",
        "        plt.legend(selected_country)\n",
        "        plt.title(col)\n",
        "        plt.show()\n",
        "    #DS.to_csv(LIWCcol+'median User money.csv') \n",
        "for LIWCcol in other:\n",
        "    colneeded=[]\n",
        "    colneeded.extend(eval(LIWCcol))\n",
        "    for col in colneeded:\n",
        "        plt.figure(figsize=(10,10))\n",
        "        for cou in selected_country:\n",
        "            plt.hist(T_COUN[T_COUN['CountryCode']==cou][col],alpha=0.7,bins=100)\n",
        "        plt.legend(selected_country)\n",
        "        plt.title(col)\n",
        "        plt.show()"
      ]
    },
    {
      "source": [
        "#### LIWC analysis on developed and developing countries\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "T_developed_developing=T[(T['CountryCode'].isin(developed_country)) | (T['CountryCode'].isin(developing_country))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "T_developed_developing['CountryType']=''\n",
        "T_developed_developing.loc[T['CountryCode'].isin(developed_country),'CountryType']='developed'\n",
        "T_developed_developing.loc[T['CountryCode'].isin(developing_country),'CountryType']='developing'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 20 words for topic #0:\n['protect', 'purchase', 'cause', 'cook', 'livestock', 'animal', 'leftover', 'consume', 'consumer', 'buy', 'reduce', 'packaging', 'compost', 'surplus', 'zerofoodwasteinitiative', 'waste', 'day', '30', 'endhunger', 'food']\n\n\nTop 20 words for topic #1:\n['temperature', 'packaging', 'activity', 'purchase', 'planning', 'little', 'waste', 'result', 'shopping', 'grocery', 'sustainable', 'spoilage', 'label', 'meal', 'date', '30', 'day', 'endhunger', 'zerofoodwasteinitiative', 'food']\n\n\n**************************************************\n"
          ]
        }
      ],
      "source": [
        "T_UK_sdg=PrintTopic(T[(T['CountryCode']=='Nigeria') & (T_developed_developing['extended_tweet'].str.contains('sdg2'))], topicnum=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(872, 73)\nit is going to take a global effort to fight the many threats we face now. we must all work together to build lasting change for the worlds most vulnerable families.  @un sg @antonioguterres explained it well during todays panel at @wfps executive board. globalgoals sdg2 \n\n\nit is important that post covid19, programmes aimed at achieving sdg2 should be more resilient. africa4nature newdealfornature forpeopleforplanet buildbackbetter africabuildbackbetter\n\n\nday 15 of 30  handling food surplus  though food production has increased in most part of the world, people do not have equal access to food.  @zerofoodwastein  sparetosave endfoodwaste endhunger sdg2 sdg12 fao zerofoodwasteinitiative \n****************************************************************************************************\n\"so excited to be working with you again kolade! ysma \" meet some of theyouth social media ambassadors for @ictforag 2020    ictforag2020  ysma  sdg2 \n\n\nthe european sustainable development report 2020 will be launched today, dec 8th! the esdr2020 uses the sdgs to guide the covid19 recovery and reports on the progress of the eu and its member states towards the sdgs. @iisd_sdgs @vonderleyen @adamrogers2030 @sdg2030 @sdgaction \n\n\n\"innovative local ideas that contribute to a more efficient and sustainable agricultural sector: the @giz_gmbh agricultural innovation fund (aif) presents the top 15 projects of the gizinnovationchallenge on tuesday, 27 october, 13:30 cet. \" livestream : tune in to the finale of the gizinnovationchallenge on tuesday, 27 october, 13:30 cet! watch the top 15 innovators present their ideas that contribute to a more efficient and sustainable agricultural sector   sdg2 \n****************************************************************************************************\n"
          ]
        }
      ],
      "source": [
        "print(T_UK_sdg.shape)\n",
        "for i in range(2):\n",
        "    print(*T_UK_sdg[T_UK_sdg['HateTopic']==i]['extended_tweet'].drop_duplicates().sample(3).tolist(),sep='\\n'*3)\n",
        "    print('*'*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('#sdg2', 834),\n",
              " ('#endfoodwaste', 747),\n",
              " ('#sparetosave', 739),\n",
              " ('#sdg12', 739),\n",
              " ('#fao', 739),\n",
              " ('#endhunger', 689),\n",
              " ('#zerofoodwasteinitiative', 491),\n",
              " ('#zerohunger', 33),\n",
              " ('#ictforag2020', 22),\n",
              " ('#sdgs', 21),\n",
              " ('#ysma', 16),\n",
              " ('#agtech', 13),\n",
              " ('#foodsecurity', 10),\n",
              " ('#sdg1', 10),\n",
              " ('#covid19', 8),\n",
              " ('#sdg2.', 7),\n",
              " ('#love', 7),\n",
              " ('#foodsystems', 6),\n",
              " ('#povertyeradication', 6),\n",
              " ('#boysmatter', 6),\n",
              " ('#anniversary', 6),\n",
              " ('#bdpglobal2020', 5),\n",
              " ('#youthindata2020', 5),\n",
              " ('#onecgiar', 5),\n",
              " ('#socialimpact', 5),\n",
              " ('#unitednations', 5),\n",
              " ('#buildbackbetter', 5),\n",
              " ('#sdg11', 4),\n",
              " ('#urbannutrition', 4),\n",
              " ('#nigeria.', 4)]"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ],
      "source": [
        "T_UK_sdg_HT=[]\n",
        "for i in T_UK_sdg['hashtags']:\n",
        "    T_UK_sdg_HT.extend(i)\n",
        "sorted(Counter(T_UK_sdg_HT).items(), key=lambda x: x[1],reverse=True)[:30]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Percentage(x):\n",
        "        return round(len(x.dropna())/len(x),4)*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Mean(x):\n",
        "    return round(x.mean(),2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def GenMeanPercen(df,selected_direction,needcol):\n",
        "    DS=[]\n",
        "    for LIWCcol in selected_direction:\n",
        "        colneeded=[needcol,'id']\n",
        "        colneeded.extend(eval(LIWCcol))\n",
        "        if LIWCcol=='AffectWords':\n",
        "            aggdict={i:[Mean,Percentage] for i in eval(LIWCcol)}\n",
        "        else:\n",
        "            aggdict={i:[CusMean] for i in eval(LIWCcol)}\n",
        "        DS_temp=df[colneeded].groupby(needcol).agg(aggdict)\n",
        "        DS_ls=[]\n",
        "        for DS_col in DS_temp.columns:\n",
        "            #print((LIWCcol,)+DS_col)\n",
        "            DS_ls.append((LIWCcol,)+DS_col)\n",
        "        DS_temp.columns=pd.MultiIndex.from_tuples(DS_ls)\n",
        "        DS.append(DS_temp.T)\n",
        "    DS_verticle=pd.concat(DS)\n",
        "    '''DS_verticle['developed']=DS_verticle['developed'].round(2)\n",
        "    DS_verticle['developing']=DS_verticle['developing'].round(2)\n",
        "    len_developed=sum(T_developed_developing['CountryType']=='developed')\n",
        "    len_developing=sum(T_developed_developing['CountryType']=='developing')\n",
        "    for i in DS_verticle.index:\n",
        "        if 'count' in i:\n",
        "            print(round(DS_verticle.loc[i,'developed']/len_developed*100,4),'developed')\n",
        "            print(round(DS_verticle.loc[i,'developing']/len_developing*100,4),'developing')\n",
        "            DS_verticle.loc[i,'developing']=round(DS_verticle.loc[i,'developing']/len_developing*100,4)\n",
        "            DS_verticle.loc[i,'developed']=round(DS_verticle.loc[i,'developed']/len_developed*100,4)'''\n",
        "\n",
        "    return DS_verticle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "C_combine=GenMeanPercen(T_developed_developing,selected_direction,'CountryType').join(GenMeanPercen(T_COUN,selected_direction,'CountryCode'))\n",
        "C_combine=C_combine[['developed', 'developing', 'Australia','United Kingdom', 'United States', 'India', 'Nigeria']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "C_combine.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "C_combine.loc[('AffectWords','sad','Mean')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#C_combine.to_csv('all analysis developing and developed percentage.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#all of the data is not normally distributed\n",
        "'''for LIWCcol in ['AffectWords','CognetiveProcess','PerpetualProcesses','BiologicalProcesses','CoreDrivesandNeeds','TimeOrientation','Relativity','PersonalConcerns','InformalSpeech','Customized','SummaryVariable']:\n",
        "    for i in eval(LIWCcol):\n",
        "        for coun in ['developed','developing']:\n",
        "            print(LIWCcol,'shapiro',coun,stats.shapiro(T_developed_developing[T_developed_developing['CountryType']==coun][i]))'''\n",
        "#create a stats table\n",
        "OneFrame=[]\n",
        "for LIWCcol in selected_direction:\n",
        "    for i in eval(LIWCcol):\n",
        "        CounCom=combinations(['developed','developing'],2)\n",
        "        for couns in CounCom:\n",
        "            st=stats.ttest_ind(T_developed_developing[T_developed_developing['CountryType']==couns[0]][i].dropna(),T_developed_developing[T_developed_developing['CountryType']==couns[1]][i].dropna(),equal_var=False)\n",
        "            OneRow=[LIWCcol,i,couns[0],couns[1],st[0],st[1]]\n",
        "            OneFrame.append(OneRow)\n",
        "            #print(i,':',couns,stats.ranksums(T_COUN[T_COUN['CountryCode']==couns[0]][i],T_COUN[T_COUN['CountryCode']==couns[1]][i])[1])\n",
        "stats_table=pd.DataFrame(OneFrame,columns=['keyword1','keyword2','coun1','coun2','test_stats','p-value'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stats_table1=stats_table[(stats_table['p-value']<0.01) & (stats_table['p-value']>=0.001)].sort_values('p-value')\n",
        "stats_table1['coun']=[frozenset([stats_table1.loc[i,'coun1'],stats_table1.loc[i,'coun2']]) for i in stats_table1.index]\n",
        "stats_table2=stats_table1.groupby('coun').agg({'keyword1':','.join,'keyword2':','.join})\n",
        "stats_table2['kw1list']=stats_table2['keyword1'].apply(lambda x: set(x.split(',')))\n",
        "stats_table2['kw2list']=stats_table2['keyword2'].apply(lambda x: set(x.split(',')))\n",
        "\n",
        "stats_table2['kw1len']=stats_table2['kw1list'].apply(lambda x:len(x))\n",
        "stats_table2['kw2len']=stats_table2['kw2list'].apply(lambda x:len(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stats_table1.sort_values('keyword1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#stats_table.to_csv('User stats table based on money.csv')"
      ]
    },
    {
      "source": [
        "○ Affiliation\t-- McClelland-like\tdimensions\tincluding\treference\tto\tothers\n",
        "\n",
        "○ Achievement\t-- references\tto\tsuccess\tand\tfailure,\tachievement\tstriving\n",
        "\n",
        "○ Power\t-- references\trelevant\tto\tstatus,\tdominance,\tsocial\thierarchies\n",
        "\n",
        "○ Reward\tfocus\t-- references\tto\trewards,\tincentives,\tpositive\tgoals,\tapproach\n",
        "\n",
        "○ Risk\tfocus\t-- references\tto\tdangers,\tconcerns,\tthings\tto\tavoid\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "##negmo is complain and disapointment. \n",
        "#Need for power — driven by prestige, control, status, and influence over others\n",
        "#Need for achievement — driven by succeeding, accomplishing goals and overcoming challenges\n",
        "#Need for affiliation — motivated by close relationships with others\n",
        "#achievement is \n",
        "# power: powerful figures, law\n",
        "print(*T_developed_developing[(T_developed_developing['leisure']>10) & (T_developed_developing['extended_tweet'].str.contains('sdg'))]['extended_tweet'].sample(10).tolist(),sep='\\n'*3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stats_table1.sort_values('keyword1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DS=[]\n",
        "for LIWCcol in selected_direction:\n",
        "    colneeded=['CountryCode','CountryType','id']\n",
        "    colneeded.extend(eval(LIWCcol))\n",
        "    aggdict={i:['mean','count'] for i in eval(LIWCcol)}\n",
        "    DS_temp=T_developed_developing[colneeded].groupby('CountryType').agg(aggdict)\n",
        "    DS_ls=[]\n",
        "    for DS_col in DS_temp.columns:\n",
        "        #print((LIWCcol,)+DS_col)\n",
        "        DS_ls.append((LIWCcol,)+DS_col)\n",
        "    DS_temp.columns=pd.MultiIndex.from_tuples(DS_ls)\n",
        "    DS.append(DS_temp.T)\n",
        "DS_verticle=pd.concat(DS)\n",
        "DS_verticle['developed']=DS_verticle['developed'].round(2)\n",
        "DS_verticle['developing']=DS_verticle['developing'].round(2)\n",
        "len_developed=sum(T_developed_developing['CountryType']=='developed')\n",
        "len_developing=sum(T_developed_developing['CountryType']=='developing')\n",
        "for i in DS_verticle.index:\n",
        "    if 'count' in i:\n",
        "        print(round(DS_verticle.loc[i,'developed']/len_developed*100,4),'developed')\n",
        "        print(round(DS_verticle.loc[i,'developing']/len_developing*100,4),'developing')\n",
        "        DS_verticle.loc[i,'developing']=round(DS_verticle.loc[i,'developing']/len_developing*100,4)\n",
        "        DS_verticle.loc[i,'developed']=round(DS_verticle.loc[i,'developed']/len_developed*100,4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#DS_verticle.to_csv('all analysis developing and developed percentage.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for LIWCcol in selected_direction:\n",
        "    colneeded=['CountryCode','CountryType','id']\n",
        "    colneeded.extend(eval(LIWCcol))\n",
        "    aggdict={i:['mean','count'] for i in eval(LIWCcol)}\n",
        "    DS=T_developed_developing[colneeded].groupby('CountryType').agg(aggdict)\n",
        "    DS_ls=[]\n",
        "    for DS_col in DS.columns:\n",
        "        #print((LIWCcol,)+DS_col)\n",
        "        DS_ls.append((LIWCcol,)+DS_col)\n",
        "    DS.columns=pd.MultiIndex.from_tuples(DS_ls)\n",
        "    display(DS)\n",
        "    #DS.to_csv(LIWCcol+'median User money.csv') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for LIWCcol in selected_direction:\n",
        "    colneeded=[]\n",
        "    colneeded.extend(eval(LIWCcol))\n",
        "    for col in colneeded:\n",
        "        fig, ax1 = plt.subplots(figsize=(10,10))\n",
        "        ax2 = ax1.twinx()\n",
        "        ax1.hist(np.log(T_developed_developing[T_developed_developing['CountryType']=='developing'][col]),alpha=0.7,bins=50,label='developing')\n",
        "        ax2.hist(np.log(T_developed_developing[T_developed_developing['CountryType']=='developed'][col]),fc=(1, 0, 0, 0.95),alpha=0.7,bins=50)\n",
        "        ax1.legend()\n",
        "        #ax1.set_ylabel('Y1 data', color='g')\n",
        "        #ax2.set_ylabel('Y2 data', color='b')\n",
        "        plt.xlim(xmin=0, xmax = 5)\n",
        "        plt.title(col)\n",
        "        plt.show()\n",
        "    #DS.to_csv(LIWCcol+'median User money.csv') \n",
        "for LIWCcol in other:\n",
        "    colneeded=[]\n",
        "    colneeded.extend(eval(LIWCcol))\n",
        "    for col in colneeded:\n",
        "        fig, ax1 = plt.subplots(figsize=(10,10))\n",
        "        ax2 = ax1.twinx()\n",
        "        ax1.hist(T_developed_developing[T_developed_developing['CountryType']=='developing'][col],alpha=0.7,bins=50,label='developing')\n",
        "        ax2.hist(T_developed_developing[T_developed_developing['CountryType']=='developed'][col],fc=(1, 0, 0, 0.95),alpha=0.7,bins=50)\n",
        "        ax1.legend()\n",
        "        #ax1.set_ylabel('Y1 data', color='g')\n",
        "        #ax2.set_ylabel('Y2 data', color='b')\n",
        "        plt.title(col)\n",
        "        plt.show()\n"
      ]
    },
    {
      "source": [
        "### Strong against SDGs"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "e0iBNWD5Hlbg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1wcWCBKAis2"
      },
      "source": [
        "def PrintTopic(df, topicnum=5,keywds=20):\n",
        "  #parameter set-up\n",
        "  count_vect=CountVectorizer(max_df=0.8,min_df=2,stop_words='english')\n",
        "  LDA = LatentDirichletAllocation(n_components=topicnum, random_state=42)\n",
        "  #topic modeling\n",
        "  tweet_matrix=count_vect.fit_transform(df['extended_tweet_lemmatized'].values.astype('U'))\n",
        "  LDA.fit(tweet_matrix)\n",
        "  #print results\n",
        "  '''  for i in top_topic_words:\n",
        "      print(count_vect.get_feature_names()[i])'''\n",
        "  for i,topic in enumerate(LDA.components_):\n",
        "      print(f'Top {keywds} words for topic #{i}:')\n",
        "      print([count_vect.get_feature_names()[i] for i in topic.argsort()[-keywds:]])\n",
        "      print('\\n')\n",
        "  print('*'*50)\n",
        "  #put topic back to tweets\n",
        "  topic_values = LDA.transform(tweet_matrix)\n",
        "  df['HateTopic']=None\n",
        "  #putinto the main matrix\n",
        "  df.loc[df.index,'HateTopic']=topic_values.argmax(axis=1)\n",
        "  return df\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(T[T['compound']>0.9]),len(T[T['compound']<-0.5])"
      ]
    },
    {
      "source": [
        "#### super positive ones"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "T_pos9=T[T['compound']>0.9]\n",
        "T_pos9=PrintTopic(T_pos9,topicnum=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#T_pos9.to_csv('User positive larger 0.9.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "T_pos9=addsdg(T_pos9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#has sdg in row\n",
        "len(T_pos9[T_pos9['SDG'].apply(lambda x:len(x))>0]),len(T_pos9)\n",
        "#rank\n",
        "T_pos9_SDG=[]\n",
        "for i in T_pos9['SDG'].to_list():\n",
        "    T_pos9_SDG.extend(i)\n",
        "sorted(Counter(T_pos9_SDG).items(),key=lambda x:x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "T_pos9[T_pos9['SDG'].apply(lambda x:'SDG16' in x)]['extended_tweet'].sample(1).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "    print('Topic',str(i),':\\n')\n",
        "    print(*T_pos9[T_pos9['HateTopic']==i]['extended_tweet'].sample(3).tolist(),sep='\\n'+'-'*30+'\\n')\n",
        "    print('\\n'+'*'*50+'\\n')"
      ]
    },
    {
      "source": [
        "#### super negative ones"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(*T[T['compound']<-0.5]['extended_tweet'].sample(5).tolist(),sep='\\n'+'*'*50+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "T_neg=T[T['compound']<-0.5]\n",
        "T_neg=PrintTopic(T_neg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#T_neg.to_csv('User negative smaller 0.5.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "T_neg=addsdg(T_neg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#lenth of rows has sdg\n",
        "print(len(T_neg[T_neg['SDG'].apply(lambda x:len(x))>0]),len(T_neg))\n",
        "#rank sdg\n",
        "T_neg_SDG=[]\n",
        "for i in T_neg['SDG'].to_list():\n",
        "    T_neg_SDG.extend(i)\n",
        "sorted(Counter(T_neg_SDG).items(),key=lambda x:x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "T_neg[T_neg['SDG'].apply(lambda x:'SDG13' in x)]['extended_tweet'].sample(1).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "    print('Topic',str(i),':\\n')\n",
        "    print(*T_neg[T_neg['HateTopic']==i]['extended_tweet'].sample(3).tolist(),sep='\\n'+'-'*30+'\\n')\n",
        "    print('\\n'+'*'*50+'\\n')"
      ]
    },
    {
      "source": [
        "#### both pos and neg, but focus on SDG\n",
        "people feel disappointed or happy about one specific SDGs"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "T_extreme=pd.concat([T_neg,T_pos9]).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def HappyorNot_SDG(df):\n",
        "    b,n=[],[]\n",
        "    for i in range(1,18,1):\n",
        "        name='SDG'+str(i)\n",
        "        b.append(sum(df[df['SDG'].apply(lambda x: name in x)]['compound']>0)/sum(df[df['SDG'].apply(lambda x: name in x)]['compound']<0)\n",
        "        n.append(name)\n",
        "    plt.bar(n,b)\n",
        "    plt.plot(range(1,18,1),np.ones(17)*sum(df['compound']>0)/sum(df['compound']<0),'red')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "source": [
        "#### select manually"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjQC5YxnJe6k"
      },
      "source": [
        "'''StrongAgainst=T[T['extended_tweet'].str.contains('propaganda|nosdgs|hypocrite|stopsdgs|globalism|hypocrisy|plandemic|pcrtestpandemic|thegreatreset|scamdemic|pcrtestpandemic|itsascam|nowef|nonsa|noimf|nocia|stopthegreatreset|noagenda21|noun |nog30',case=False)].drop_duplicates(subset='extended_tweet')#['extended_tweet']#.drop_duplicates()'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''StrongAgainst=T[T['extended_tweet'].str.contains('propaganda|nosdgs|hypocrite|stopsdgs|globalism|hypocrisy|itsascam|nowef|noagenda21|nog30',case=False)].drop_duplicates(subset='extended_tweet')#['extended_tweet']#.drop_duplicates()'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "StrongAgainst=T[T['extended_tweet'].str.contains('nosdgs|noagenda21|nog30|agenda2030 propaganda|stop agenda21|lie of sdgs|lie of sustainabledevelopment|fuck agenda21|force agenda2030|sdgs is a lie|climateaction scam|lie of sustainabledevelopment',case=False)].drop_duplicates(subset='extended_tweet')#['extended_tweet']#.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#StrongAgainst=StrongAgainst.append(T_neg[T_neg['extended_tweet'].str.contains('agenda21|agenda30')]).drop_duplicates(subset='extended_tweet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "StrongAgainst['compound'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(*StrongAgainst['extended_tweet'].sample(25).tolist(),sep='\\n'+'-'*30+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iivJjSQoL3EQ"
      },
      "source": [
        "StrongAgainst_HT=[]\n",
        "for i in StrongAgainst['hashtags'].tolist():\n",
        "  StrongAgainst_HT.extend(i)\n",
        "StrongAgainst_HT=Counter(StrongAgainst_HT)\n",
        "StrongAgainst_HT={i:k for i,k in sorted(StrongAgainst_HT.items(),key=lambda x:x[1],reverse=True)}\n",
        "#del StrongAgainst_HT['#sdg']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9glb4PUwFG8T"
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.bar(list(StrongAgainst_HT.keys())[:30],list(StrongAgainst_HT.values())[:30])\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "list(StrongAgainst_HT.keys())[:30]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwNhGGQFXcU7"
      },
      "source": [
        "topnum=4\n",
        "StrongAgainst=PrintTopic(StrongAgainst,topnum,20)\n",
        "\n",
        "for i in range(topnum):\n",
        "  print(*StrongAgainst[StrongAgainst['HateTopic']==i].sample(2)['extended_tweet'].tolist(),sep='\\n'+'-'*100+'\\n')\n",
        "  print('*'*100)\n",
        "\n",
        "plt.hist(StrongAgainst['HateTopic'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(*T[T['extended_tweet'].str.contains('firetrudeau')]['extended_tweet'].drop_duplicates().tolist(),sep='\\n'*3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#StrongAgainst.to_csv('User StrongAgainst 42.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "HT=[]\n",
        "for i in StrongAgainst[StrongAgainst['HateTopic']==1]['hashtags'].tolist():\n",
        "    HT.extend(i)\n",
        "sorted(Counter(HT).items(),key=lambda x:x[1],reverse=True)"
      ]
    },
    {
      "source": [
        "#### Plandemic related"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Plandemic=T[T['extended_tweet'].str.contains('nosdgs|noagenda21|nog30|agenda2030 propaganda|stop agenda21|lie of sdgs|lie of sustainabledevelopment|fuck agenda21|force agenda2030|sdgs is a lie|plandemic|lie of sustainabledevelopment|nogreatreset|nocia|noglobalism|noimf|noworldbank|nowef|vaxaware|nonewnormal|nonsa|novaccine|nomask|nocia|nofbi|noreset|nolockdown|scamdemic|nonwo',case=False)].drop_duplicates(subset='extended_tweet')#['extended_tweet']#.drop_duplicates()\n",
        "\n",
        "Plandemic=Plandemic.append(T[(T['extended_tweet'].str.contains('propaganda')) & (T['extended_tweet'].str.contains('agenda'))].drop_duplicates(subset='extended_tweet'))\n",
        "Plandemic=Plandemic.append(T[(T['extended_tweet'].str.contains('hypoc')) & (T['extended_tweet'].str.contains('agenda'))].drop_duplicates(subset='extended_tweet'))\n",
        "Plandemic=Plandemic.append(T[(T['extended_tweet'].str.contains('hypoc')) & (T['extended_tweet'].str.contains('sdg'))].drop_duplicates(subset='extended_tweet'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Plandemic=Plandemic.drop_duplicates(subset='extended_tweet')\n",
        "Plandemic['old_tweet']=Plandemic['extended_tweet']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(*T[(T['extended_tweet'].str.contains('nonwo')) & (T['extended_tweet'].str.contains(''))]['extended_tweet'].drop_duplicates().tolist(),sep='\\n'*3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''plandemic_HT=[]\n",
        "for i in Plandemic['hashtags'].tolist():\n",
        "  plandemic_HT.extend(i)\n",
        "plandemic_HT=Counter(plandemic_HT)\n",
        "plandemic_HT={i:k for i,k in sorted(plandemic_HT.items(),key=lambda x:x[1],reverse=True)}\n",
        "plandemic_HT\n",
        "#del StrongAgainst_HT['#sdg']'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "for i in Plandemic.index:\n",
        "    tweet=Plandemic.loc[i,'extended_tweet']\n",
        "    for ii in Plandemic.loc[i,'hashtags']:\n",
        "        tweet=tweet.replace(ii[1:],'')\n",
        "    Plandemic.loc[i,'extended_tweet']=tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "l=['scamdemic', 'people', 'refuse', 'plandemic', 'propaganda', 'agenda21', 'covid19', 'lie', 'agenda2030', 'sdgs']\n",
        "l.reverse()\n",
        "print(', '.join(l))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topnum=2\n",
        "Plandemic=PrintTopic(Plandemic,topnum,10)\n",
        "\n",
        "for i in range(topnum):\n",
        "  print(*Plandemic[Plandemic['HateTopic']==i].sample(2)['old_tweet'].tolist(),sep='\\n'+'-'*100+'\\n')\n",
        "  print('*'*100)\n",
        "\n",
        "plt.hist(Plandemic['HateTopic'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "P_topic_0,P_topic_1=[],[]\n",
        "for i in Plandemic[Plandemic['HateTopic']==0]['hashtags'].tolist():\n",
        "    P_topic_0.extend(i)\n",
        "for i in Plandemic[Plandemic['HateTopic']==1]['hashtags'].tolist():\n",
        "    P_topic_1.extend(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(sorted(Counter(P_topic_0).items(),key=lambda x:x[1],reverse=True)[:15])\n",
        "print('\\n'*3)\n",
        "print(sorted(Counter(P_topic_1).items(),key=lambda x:x[1],reverse=True)[:15])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Plandemic.info()"
      ]
    },
    {
      "source": [
        "#### Network plot"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "AgainstHT=[]\n",
        "for i in StrongAgainst['hashtags'].tolist():\n",
        "    AgainstHT.extend(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "GC = nx.Graph()\n",
        "GC.add_nodes_from(set(AgainstHT))\n",
        "for hash_list in StrongAgainst.sort_values('created_at')['hashtags']:\n",
        "    GC.add_edges_from(combinations(hash_list,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "plt.figure(figsize=(20,20))\n",
        "# find node with largest degree\n",
        "node_and_degree = GC.degree()\n",
        "(largest_hub, degree) = sorted(node_and_degree, key=itemgetter(1))[-1]\n",
        "\n",
        "# Create ego graph of main hub\n",
        "hub_ego = nx.ego_graph(GC, largest_hub)\n",
        "\n",
        "# Draw graph\n",
        "pos = nx.spring_layout(hub_ego)\n",
        "nx.draw(hub_ego, pos, node_color=\"b\", node_size=50, with_labels=True,font_weight='bold')\n",
        "\n",
        "# Draw ego as large and red\n",
        "\n",
        "options = {\"node_size\": 300, \"node_color\": \"r\"}\n",
        "nx.draw_networkx_nodes(hub_ego, pos, nodelist=[largest_hub], **options)\n",
        "#plt.savefig('User StrongAgainst Network.png',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#nx.write_graphml(GC, '42 StrongAgainst Network.graphml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "degree_sequence = sorted([d for n, d in G.degree()], reverse=True)\n",
        "dmax = max(degree_sequence)\n",
        "\n",
        "plt.loglog(degree_sequence, \"b-\", marker=\"o\")\n",
        "plt.title(\"Degree rank plot\")\n",
        "plt.ylabel(\"degree\")\n",
        "plt.xlabel(\"rank\")\n",
        "\n",
        "# draw graph in inset\n",
        "#plt.axes([0.45, 0.45, 0.45, 0.45])\n",
        "plt.figure(figsize=(10,5))\n",
        "Gcc = G.subgraph(sorted(nx.connected_components(G), key=len, reverse=True)[0])\n",
        "pos = nx.spring_layout(Gcc)\n",
        "plt.axis(\"off\")\n",
        "nx.draw_networkx_nodes(Gcc, pos, node_size=20)\n",
        "nx.draw_networkx_edges(Gcc, pos, alpha=0.4)\n",
        "plt.show()"
      ]
    },
    {
      "source": [
        "#### negative with asdgs"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "T[(T['extended_tweet'].str.contains('sdg')) & (T['extended_tweet'].str.contains('ourmoney'))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(*T[(T['extended_tweet'].str.contains('trudeau')) & (T['extended_tweet'].str.contains('scandal'))]['extended_tweet'].drop_duplicates().tolist(),sep='\\n'+'*'*50+'\\n')"
      ]
    },
    {
      "source": [
        "#### strong against only contains nosdg and etc hashtags"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Allhash=[]\n",
        "for k,i in enumerate(StrongAgainst['hashtags']):\n",
        "    if isinstance(i,float):\n",
        "        StrongAgainst.drop(k,inplace=True)\n",
        "        print(k,i)\n",
        "    else:\n",
        "        Allhash.extend(i)\n",
        "import networkx as nx\n",
        "AllHT = nx.Graph()\n",
        "AllHT.add_nodes_from(set(Allhash))\n",
        "\n",
        "for hash_list in StrongAgainst.sort_values('created_at')['hashtags']:\n",
        "    AllHT.add_edges_from(combinations(hash_list,2))\n",
        "\n",
        "nx.write_graphml(AllHT, 'StrongAgainst nosdgs only Network.graphml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "','.join(['SDG4', 'SDG7', 'SDG16', 'SDG3', 'SDG5', 'SDG2', 'SDG13', 'SDG6', 'SDG14', 'SDG10', 'SDG1', 'SDG12', 'SDG17'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}