{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SDGs (1).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "N3ZLvFrM7rRw",
        "jQybSTqP9ANi"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.3 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3ZLvFrM7rRw"
      },
      "source": [
        "# Import Data\n",
        "## import and install important libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhN_GSxKmKrp"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "from os import listdir\n",
        "from os.path import join\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOyArWcq2jp6"
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kVfAU1m387X"
      },
      "source": [
        "import emoji#checking if a character is an emoji\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import word_tokenize \n",
        "from nltk.corpus import stopwords \n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsNFuzmP2jp6"
      },
      "source": [
        "T=pd.read_csv('/Users/livi/Git/Tweepy-data-mining/LIWC2015 Results (T12292020).csv', index_col=0,lineterminator='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMgF-U0v2jp7"
      },
      "source": [
        "#rename columnsT\n",
        "T_cols=list(T.columns)\n",
        "T_cols[:18]=['created_at', 'source', 'extended_tweet', 'location', 'CountryCode','SDG', 'id', 'name', 'screen_name', 'url', 'friends_count','followers_count', 'hashtags', 'extended_tweet_lemmatized', 'neg','neu', 'pos', 'compound']\n",
        "T.columns=T_cols\n",
        "\n",
        "# LIWC columns\n",
        "AffectWords=['affect','posemo', 'negemo', 'anx', 'anger', 'sad']\n",
        "CognetiveProcess=['cogproc','insight', 'cause', 'discrep', 'tentat', 'certain', 'differ']\n",
        "PerpetualProcesses=['percept','see', 'hear', 'feel']\n",
        "BiologicalProcesses=['bio', 'body', 'health', 'sexual', 'ingest']\n",
        "CoreDrivesandNeeds=['drives','affiliation', 'achieve', 'power', 'reward', 'risk']\n",
        "TimeOrientation=['focuspast', 'focuspresent', 'focusfuture']\n",
        "Relativity=['relativ','motion', 'space', 'time']\n",
        "PersonalConcerns=['work', 'leisure', 'home', 'money', 'relig', 'death']\n",
        "InformalSpeech=['informal','swear','netspeak','assent','nonflu','filler']\n",
        "PersonalPronouns=['ppron','i','we','you','shehe','they']\n",
        "SummaryVariable=['Analytic','Clout','Authentic','Tone']\n",
        "Customized=['neg','pos','compound']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgm2J9VT2jp7"
      },
      "source": [
        "#drop ['location','name','url']\n",
        "desired_col=['created_at', 'source', 'extended_tweet', 'CountryCode','SDG', 'id', 'screen_name','followers_count', 'hashtags', 'extended_tweet_lemmatized','neu', 'neg','pos', 'compound']\n",
        "for i in ['AffectWords','CognetiveProcess','PerpetualProcesses','BiologicalProcesses','CoreDrivesandNeeds','TimeOrientation','Relativity','PersonalConcerns','InformalSpeech','PersonalPronouns','SummaryVariable']:\n",
        "  desired_col.extend(eval(i))\n",
        "T=T[desired_col]\n",
        "T.drop(index=T[pd.isnull(T['extended_tweet'])].index,inplace=True)\n",
        "T.drop(index=T[pd.isnull(T['id'])].index,inplace=True)\n",
        "T.reset_index(drop=True,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lgfYIWY2jp8"
      },
      "source": [
        "#Country code clean\n",
        "for i,k in enumerate(T['CountryCode']):\n",
        "    if ']' in str(k):\n",
        "        T.loc[i,'CountryCode']='nan'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9xbxqE86jXe"
      },
      "source": [
        "#str\n",
        "for col in ['source','extended_tweet','CountryCode','id','extended_tweet_lemmatized']:\n",
        "    T[col]=T[col].astype(str)\n",
        "#apply\n",
        "for col in ['SDG','hashtags']:\n",
        "    T[col]=T[col].apply(eval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVDvH1V-2jp8"
      },
      "source": [
        "T['extended_tweet']=T['extended_tweet'].str.replace('\\n',' ')\n",
        "T['extended_tweet']=T['extended_tweet'].str.replace('#','')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCPuqJPE2jp8"
      },
      "source": [
        "for i in T.columns:\n",
        "    if T[i].dtypes=='float64':\n",
        "        T[i].replace(0,np.nan,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EWm92XB2jp9"
      },
      "source": [
        "for n,i in enumerate(T['SDG']):\n",
        "    if isinstance(i,float):\n",
        "        T.drop(index=n,inplace=True)\n",
        "T.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzEZaiiR2jp9"
      },
      "source": [
        "T.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SFaRepraql4"
      },
      "source": [
        "# Data Cleaning\n",
        "## Generate user information\n",
        "### Detect Bot and Consider label Bot\n",
        "1. interarrival time is smaller than 5 seconds(Tested, mainly small intervals are caused by long tweets)\n",
        "2. other weird resource (source from bot may have interarrival time much longer 5 sec) Most effective method\n",
        "3. number of tweets per day (statuses_count, not used here, because there are accounts which frequently tweets)\n",
        "\n",
        "number of Influencer:A micro-influencer is someone who has between 1,000 to 100,000 followers\n",
        "\n",
        "Not include other SDG according to the keywords, otherwise it becomes misleading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6UmU_dl6jXf"
      },
      "source": [
        "#Assign user type\n",
        "T['UserType']=''\n",
        "screen_name_of_bots=list(T[T['source'].str.contains('bot')]['screen_name'].unique())\n",
        "screen_name_of_bots.extend(['trendsinAI','form_bot','UDHROne257_247','globalhealthbot'])\n",
        "screen_name_of_bots.extend(list(T[T['screen_name'].str.contains('retweet',case=False)]['screen_name'].unique()))\n",
        "BFF=T[['screen_name','followers_count']].groupby(['screen_name']).agg({'followers_count':'max'}).reset_index()\n",
        "#assign user\n",
        "screen_name_of_user=list(BFF[(BFF['followers_count']<=800)]['screen_name'].unique())\n",
        "T.loc[(T['screen_name'].isin(screen_name_of_user))& (~ T['screen_name'].isin(screen_name_of_bots)),'UserType']='user'\n",
        "#assign influencer\n",
        "screen_name_of_influencer=list(BFF[(BFF['followers_count']>800)]['screen_name'].unique())\n",
        "screen_name_of_influencer.extend(list(T[T['screen_name'].str.contains('news',case=False)]['screen_name'].unique()))\n",
        "T.loc[(T['screen_name'].isin(screen_name_of_influencer)) & (~ T['screen_name'].isin(screen_name_of_bots)),'UserType']='influencer'\n",
        "#assign bots\n",
        "T.loc[T['screen_name'].isin(screen_name_of_bots),'UserType']='bot'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWJjOec32jp-"
      },
      "source": [
        "T=T[T['UserType']=='user'].reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRW6MXuT2jp_"
      },
      "source": [
        "T.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kg5kfxI2jp_"
      },
      "source": [
        "### Select Countries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4CiVPMx2jp_"
      },
      "source": [
        "selected_country=['United States','United Kingdom','Australia','India','Nigeria']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8UspbIe2jqA"
      },
      "source": [
        "#### Developing and Developed Country Compare (vote the top then select)\n",
        "#define developed country and developing country\n",
        "developed_country='Canada,United States,Australia,Japan,New Zealand,Austria,Belgium,Denmark,Finland,France,Germany,Greece,Ireland,Italy,Luxembourg,Netherlands,Portugal,Spain,Sweden,United Kingdom,Bulgaria,Croatia,Cyprus,Czechia,Estonia,Hungary,Latvia,Lithuania,Malta,Poland,Romania,Slovakia,Slovenia,Iceland,Norway,Switzerland'.split(',')\n",
        "transition='Albania,Bosnia and Herzegovina,Montenegro,North Macedonia,Serbia,Armenia,Azerbaijan,Belarus,Georgia,Kazakhstan,Kyrgyzstan,Moldova,Russia,Tajikistan,Turkmenistan,Ukraine,Uzbekistan'.split(',')\n",
        "developing_country='Algeria,Egypt,Libya,Mauritania,Morocco,Sudan,Tunisia,Cameroon,Central African Republic,Chad,Republic of the Congo,Equatorial Guinea,Gabon,São Tomé and Príncipe,Burundi,Comoros,Democratic Republic of the Congo,Djibouti,Eritrea,Ethiopia,Kenya,Madagascar,Rwanda,Somalia,South Sudan,Uganda,Tanzania,Angola,Botswana,Eswatini,Lesotho,Malawi,Mauritius,Mozambique,Namibia,South Africa,Zambia,Zimbabwe,Benin,Burkina Faso,Cape Verde,Côte d\\'Ivoire,The Gambia,Ghana,Guinea,Guinea-Bissau,Liberia,Mali,Niger,Nigeria,Senegal,Sierra Leone,Togo,Brunei,Cambodia,China,North Korea,Fiji,Hong Kong,Indonesia,Kiribati,Laos,Malaysia,Mongolia,Myanmar (Burma),Papua New Guinea,Philippines,South Korea,Samoa,Singapore,Solomon Islands,Taiwan,Thailand,Timor-Leste,Vanuatu,Vietnam,Afghanistan,Bangladesh,Bhutan,India,Iran,Maldives,Nepal,Pakistan,Sri Lanka,Bahrain,Iraq,Israel,Jordan,Kuwait,Lebanon,Oman,Qatar,Saudi Arabia,Palestine,Syria,Turkey,United Arab Emirates,Yemen,The Bahamas,Barbados,Belize,Guyana,Jamaica,Suriname,Trinidad and Tobago,Costa Rica,Cuba,Dominican Republic,El Salvador,Guatemala,Haiti,Honduras,Mexico,Nicaragua,Panama,Argentina,Bolivia,Brazil,Chile,Colombia,Ecuador,Paraguay,Peru,Uruguay,Venezuela'.split(',')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jh6Os6A19y-"
      },
      "source": [
        "# Analysis\n",
        "## Check User, Inflencer and Bot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNFhFfuM6jXh"
      },
      "source": [
        "Users=T[['screen_name','id','UserType','source']].groupby('screen_name').agg({'id':'count','UserType':'last','source':'last'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMjklrFT6jXh"
      },
      "source": [
        "Users.sort_values('id',ascending=False,inplace=True)\n",
        "Users.reset_index(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfJs9CL36jXh"
      },
      "source": [
        "cri_bot=Users['UserType']=='bot'\n",
        "cri_user=Users['UserType']=='user'\n",
        "cri_influencer=Users['UserType']=='influencer'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIaw28JH6jXi"
      },
      "source": [
        "set(Users['screen_name'].to_list()[:30]).intersection(set(Users[cri_user]['screen_name'].to_list()[:15]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XmZdxZv6jXi"
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.bar(Users['screen_name'].to_list()[:40],Users['id'].to_list()[:40])\n",
        "plt.title('All', fontsize=15)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.bar(Users[cri_user]['screen_name'].to_list()[:30],Users[cri_user]['id'].to_list()[:30])\n",
        "plt.title('User', fontsize=15)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "'''plt.figure(figsize=(15,8))\n",
        "plt.bar(Users[cri_influencer]['screen_name'].to_list()[:15],Users[cri_influencer]['id'].to_list()[:15])\n",
        "plt.title('Influencer', fontsize=15)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgMrYeEs6jXi"
      },
      "source": [
        "T[['screen_name','id','UserType','source']].groupby('UserType').agg({'id':'count','source':'last','screen_name':'last'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzygiEN12jqC"
      },
      "source": [
        "## Frequency of SDG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91cEzJHw2jqC"
      },
      "source": [
        "TT=T[['extended_tweet', 'SDG','CountryCode']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fkpug7d72jqD"
      },
      "source": [
        "TT['length']=TT['SDG'].apply(lambda x: type(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtYRvXk82jqD"
      },
      "source": [
        "TT['length']=TT['SDG'].apply(lambda x: len(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGfHknPd2jqD"
      },
      "source": [
        "TT=TT[[len(i)>0 for i in TT['SDG']]]\n",
        "TT.reset_index(drop=True,inplace=True)\n",
        "print(TT.shape)\n",
        "TT.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4jIpoJd2jqD"
      },
      "source": [
        "#Clean up the string\n",
        "Tdict={}\n",
        "for i in TT['SDG'].to_list():\n",
        "    for ii in i:\n",
        "        if ii in Tdict:\n",
        "            Tdict[ii]+=1\n",
        "        else:\n",
        "            Tdict[ii]=1\n",
        "#Tdict['']+=Tdict['']\n",
        "try:\n",
        "  Tdict['SDG1']+=Tdict['SDG01']\n",
        "  Tdict['SDG8']+=Tdict['SDG08']\n",
        "  Tdict['SDG6']+=Tdict['SDG64']\n",
        "  Tdict['SDG4']+=Tdict['SDG04']\n",
        "  del Tdict['SDG01']\n",
        "  del Tdict['SDG04']\n",
        "  #del Tdict['SDG4ALL']\n",
        "  #del Tdict['SDG4B']\n",
        "  #del Tdict['SDG4IT']\n",
        "  #del Tdict['SDG4SURVEY']\n",
        "  #del Tdict['SDG4PT7']\n",
        "  del Tdict['SDG64']\n",
        "  del Tdict['SDG08']\n",
        "  del Tdict['SDG18']\n",
        "  del Tdict['SDG200']\n",
        "  del Tdict['SDG2030']\n",
        "  del Tdict['SDG2020']\n",
        "  del Tdict['SDG19']\n",
        "\n",
        "except:\n",
        "  #raise\n",
        "  pass\n",
        "Tdict={k:v for k,v in sorted(Tdict.items(), key=lambda x: x[1],reverse=True)}\n",
        "sorted(Tdict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGO4yD312jqE"
      },
      "source": [
        "print(Tdict.keys())\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.bar(Tdict.keys(),Tdict.values())\n",
        "plt.xlabel('SDG',fontsize=25,weight='bold')\n",
        "plt.xticks(fontsize=20,weight='bold',rotation=90)\n",
        "plt.yticks(fontsize=20,weight='bold',rotation=90)\n",
        "plt.ylabel('Frequency',fontsize=25,weight='bold')\n",
        "plt.tight_layout()\n",
        "#plt.savefig('SDG popularity.jpg',dpi=300)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQybSTqP9ANi"
      },
      "source": [
        "## Generate National Graph\n",
        "### import libraty and geojson"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zo01q7vrhVr1"
      },
      "source": [
        "import folium"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6LmhSwJhgGn"
      },
      "source": [
        "with open('/Users/livi/Git/Tweepy-data-mining/countries.geojson') as f:\n",
        "  geodata = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TWIyC4sjRzU"
      },
      "source": [
        "holdlist=[]\n",
        "allcountries=[]\n",
        "for k,i in enumerate(geodata['features']):\n",
        "  allcountries.append(i['properties']['ADMIN'])\n",
        "  if i['properties']['ADMIN'] in T['CountryCode'].tolist():\n",
        "    holdlist.append(i['properties']['ADMIN'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx8-Ft05uzTT"
      },
      "source": [
        "print('Overlap of geojson and countrycode:',len(np.unique(np.array(holdlist))))\n",
        "print('CountryCode in data:',len(T['CountryCode'].unique()))\n",
        "print('Geojson we have:',len(np.unique(allcountries)))\n",
        "df1=pd.DataFrame(allcountries)\n",
        "df1.index=df1[0]\n",
        "df1.columns=['geojson']\n",
        "df2=pd.DataFrame(T['CountryCode'].unique())\n",
        "df2.index=df2[0]\n",
        "df2.columns=['googlemap']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDY4EPyV6jXk"
      },
      "source": [
        "#countries could not be identified\n",
        "countrynotincluded=list(set(T['CountryCode'].unique())-set(np.unique(np.array(holdlist))))\n",
        "sum(T['CountryCode'].isin(countrynotincluded)),len(T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHWAzABZlRhV"
      },
      "source": [
        "### Plot Frequency Map and sccatter plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAs1yAB42jqG"
      },
      "source": [
        "PP=T[['CountryCode', 'id']].groupby('CountryCode').count()\n",
        "PP.reset_index(inplace=True)\n",
        "PP.sort_values('id',ascending=False,inplace=True)\n",
        "PP['count']=np.log10(PP['id'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBpnccR4inwu"
      },
      "source": [
        "#frequency map of tweets\n",
        "m = folium.Map(location=[0, 0], zoom_start=2)\n",
        "folium.Choropleth(\n",
        "    geo_data=geodata,\n",
        "    name='choropleth',\n",
        "    data=PP,\n",
        "    columns=['CountryCode', 'count'],\n",
        "    key_on='feature.properties.ADMIN',\n",
        "    fill_color='YlGn',\n",
        "    fill_opacity=0.7,\n",
        "    line_opacity=0.2,\n",
        "    legend_name='# of Tweets'\n",
        ").add_to(m)\n",
        "\n",
        "folium.LayerControl().add_to(m)\n",
        "m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhaFNIWU6jXl"
      },
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "plt.bar(PP.CountryCode[1:30],PP.id[1:30])\n",
        "plt.xlabel('Country',fontsize=20)\n",
        "plt.xticks(rotation=90,fontsize=15)\n",
        "plt.yticks(fontsize=15)\n",
        "plt.ylabel('Frequency',fontsize=20)\n",
        "plt.tight_layout()\n",
        "#plt.savefig('Tweets Frequency with countries.jpg',dpi=300)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khGaoa3naVH9"
      },
      "source": [
        "### plot SDG Map and heatmap table \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_UIUjUecggz"
      },
      "source": [
        "#Country and SDG distribution\n",
        "CS=T[['CountryCode','SDG']]\n",
        "SDs={}\n",
        "for coun in CS['CountryCode'].unique():\n",
        "    SDs[coun]={'SDG'+str(i+1):0 for i in range(17)}\n",
        "    temp=CS[CS['CountryCode']==coun]['SDG']\n",
        "    for S in temp:\n",
        "        for SS in S:\n",
        "            try:\n",
        "              SDs[coun][SS]+=1\n",
        "            except:\n",
        "                pass  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS9psL-52jqH"
      },
      "source": [
        "#### distribution heatmap "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnQq5FUU0g3m"
      },
      "source": [
        "SD_Coun_HM=pd.DataFrame({i:SDs[i] for i in selected_country})\n",
        "rowname,colname=SD_Coun_HM.columns,SD_Coun_HM.index\n",
        "#prepare label\n",
        "HM_label=SD_Coun_HM.div(SD_Coun_HM.sum(axis=0),axis=1).T.values\n",
        "#prepare data\n",
        "SD_Coun_HM=pd.DataFrame(preprocessing.MinMaxScaler().fit_transform(SD_Coun_HM).T)\n",
        "SD_Coun_HM.index,SD_Coun_HM.columns=rowname,colname\n",
        "#plt\n",
        "plt.figure(figsize=(20,4))\n",
        "axhm = plt.gca()\n",
        "axhm.xaxis.set_ticks_position('top')\n",
        "cmp=sns.color_palette(\"Blues\", as_cmap=True)\n",
        "sns.heatmap(SD_Coun_HM,cmap=cmp,vmax=1.2,linewidths=1,annot=HM_label,fmt='.1%',cbar=False,annot_kws={\"fontsize\":13})\n",
        "plt.xticks(fontsize=15)\n",
        "plt.yticks(fontsize=15)\n",
        "plt.savefig('HeatMap Country User.png',dpi=300)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pJR-w7D2jqH"
      },
      "source": [
        "#### Distribution of SDG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUvZ9XuC6jXl"
      },
      "source": [
        "def multi_dict_max(dict):\n",
        "    max_value=max(dict.values())\n",
        "    return [i for i in dict.keys() if dict[i]==max_value]\n",
        "def multi_dict_min(dict):\n",
        "    min_value=min(dict.values())\n",
        "    return [i for i in dict.keys() if dict[i]==min_value]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5dqzHNYayLK"
      },
      "source": [
        "#Color of each SDG\n",
        "Colors=['#e4253c','#dea73a','#4c9f45','#c5202e','#f0412b','#29bee2','#fac315','#a21c44','#f26a2c','#dd1768','#f99d27','#be8b2c','#417f45','#1c97d3','#5dbb47','#06699e','#18486b']\n",
        "SDGcolor={'SDG'+str(i+1):Colors[i] for i in range(17)}\n",
        "\n",
        "#statistic of some selected countries\n",
        "#statistic of some selected countries\n",
        "EsC_count={}\n",
        "for i in SDs.keys():\n",
        "    if i==i and i not in [\"['SDG14']\",\"[]\"]:\n",
        "        try:\n",
        "            EsC_count[i]={'sum':0,'top':0,'percentage':0}\n",
        "            EsC_count[i]['sum']=sum(SDs[i].values())\n",
        "            if EsC_count[i]['sum']!=0:\n",
        "                EsC_count[i]['percentage']=round(max(SDs[i].values())/sum(SDs[i].values())*100,1)\n",
        "            else:\n",
        "                EsC_count[i]['percentage']=0\n",
        "            EsC_count[i]['top']=multi_dict_max(SDs[i])\n",
        "            EsC_count[i]['bottom']=multi_dict_min(SDs[i])\n",
        "            #EsC_count[i]['geo']=[geodict[i]['lat'],geodict[i]['lng']]\n",
        "        except:\n",
        "            raise\n",
        "#select some countries\n",
        "ggdataEsC=geodata\n",
        "select=[]\n",
        "for i in range(len(ggdataEsC['features'])):\n",
        "    if ggdataEsC['features'][i]['properties']['ADMIN'] in EsC_count.keys():\n",
        "        select.append(i)\n",
        "ggdataEsC['features']=[ggdataEsC['features'][i] for i in select]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLIdB1U8a1-H"
      },
      "source": [
        "#plot the graph most popular\n",
        "def stfunc(features):\n",
        "    sort_features={i:Tdict[i] for i in EsC_count[features['properties']['ADMIN']]['top']}\n",
        "    sort_features=sorted(sort_features.items(),key=lambda x: x[1],reverse=True)\n",
        "    return {'fillOpacity': 0.9,'weight': 0,'fillColor':SDGcolor[sort_features[0][0]]}\n",
        "def gengeo(coun):\n",
        "    f={'features':[ggdataEsC['features'][i]] for i in range(len(ggdataEsC['features'])) if ggdataEsC['features'][i]['properties']['ADMIN']==coun}\n",
        "    f['type']='FeatureCollection'\n",
        "    return f\n",
        "\n",
        "n = folium.Map(location=[50, 10], zoom_start=2.2,tiles='OpenStreetMap')\n",
        "folium.GeoJson(ggdataEsC,name='United States',style_function=stfunc).add_to(n)\n",
        "n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_2xKYE1jNdZ"
      },
      "source": [
        "#### vote the top then select, developing and developed countries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAj1Q8CEj1Qt"
      },
      "source": [
        "def talkmost(country,T):\n",
        "  for i in list(set(country)-set(SDs.keys())):\n",
        "    country.remove(i)\n",
        "  country_SDs={i:SDs[i]for i in country}\n",
        "  country_TopSDG=[]\n",
        "  for i in country_SDs:\n",
        "    try:\n",
        "      if sum(country_SDs[i].values())>20 & max(country_SDs[i].values())>1:#make sure more than 1 tweets in the max column\n",
        "        MAXtop=max(country_SDs[i].values())\n",
        "        country_TopSDG.extend([ii for ii in country_SDs[i] if country_SDs[i][ii]==MAXtop])    \n",
        "    except:\n",
        "      pass\n",
        "  country_TopSDG={k:v for k,v in sorted(Counter(country_TopSDG).items(),key=lambda x:x[1],reverse=True)}\n",
        "  plt.bar(country_TopSDG.keys(),country_TopSDG.values())\n",
        "  plt.title(T)\n",
        "  print(list(country_TopSDG.keys()))\n",
        "  plt.show()\n",
        "\n",
        "def talkleast(country,T):\n",
        "  for i in list(set(country)-set(SDs.keys())):\n",
        "    country.remove(i)\n",
        "  country_SDs={i:SDs[i]for i in country}\n",
        "  country_TopSDG=[]\n",
        "  for i in country_SDs:\n",
        "    try:\n",
        "      if sum(country_SDs[i].values())>20:#make sure more than 1 tweets in the max column\n",
        "        MINtop=min(country_SDs[i].values())\n",
        "        country_TopSDG.extend([ii for ii in country_SDs[i] if country_SDs[i][ii]==MINtop])\n",
        "        #print(MINtop)\n",
        "    except:\n",
        "      pass\n",
        "  country_TopSDG={k:v for k,v in sorted(Counter(country_TopSDG).items(),key=lambda x:x[1],reverse=True)}\n",
        "  plt.bar(country_TopSDG.keys(),country_TopSDG.values())\n",
        "  plt.title(T)\n",
        "  print(list(country_TopSDG.keys()))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R145PSnG6jXn"
      },
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "talkmost(developed_country,'Developed talk Most')\n",
        "plt.figure(figsize=(15,5))\n",
        "talkleast(developed_country,'Developed talk Least')\n",
        "plt.figure(figsize=(15,5))\n",
        "talkmost(developing_country,'developing talk Most')\n",
        "plt.figure(figsize=(15,5))\n",
        "talkleast(developing_country,'developing talk Least')\n",
        "plt.figure(figsize=(15,5))\n",
        "talkmost(transition,'Transition talk Most')\n",
        "plt.figure(figsize=(15,5))\n",
        "talkleast(transition,'Transition talk Least')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrjJltzq_GB_"
      },
      "source": [
        "#### Based on Frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewv63om3AFaQ"
      },
      "source": [
        "def toptalkfreq(countrylist,T):\n",
        "  SD_Co=pd.DataFrame({i:SDs[i] for i in countrylist})\n",
        "  SD_Co.sum(axis=1).sort_values(ascending=False).plot(kind='bar')\n",
        "  l=list(SD_Co.sum(axis=1).sort_values(ascending=False).index)\n",
        "  #l = [i.strip('\\'\\'') for i in l]\n",
        "  print(l)\n",
        "\n",
        "  plt.title(T)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nzbc50w6jXn"
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "toptalkfreq(developed_country,'Developed Countries')\n",
        "plt.figure(figsize=(10,5))\n",
        "toptalkfreq(developing_country,'Developing Countries')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGGwEh7xcIsw"
      },
      "source": [
        "## LIWC analysis\n",
        "### meaning of columns analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIf6iiNr2jqK"
      },
      "source": [
        "def PrintTopic(df, topicnum=5,keywds=20):\n",
        "  #parameter set-up\n",
        "  count_vect=CountVectorizer(max_df=0.8,min_df=2,stop_words='english')\n",
        "  LDA = LatentDirichletAllocation(n_components=topicnum, random_state=42)\n",
        "  #topic modeling\n",
        "  tweet_matrix=count_vect.fit_transform(df['extended_tweet_lemmatized'].values.astype('U'))\n",
        "  LDA.fit(tweet_matrix)\n",
        "  #print results\n",
        "  '''  for i in top_topic_words:\n",
        "      print(count_vect.get_feature_names()[i])'''\n",
        "  for i,topic in enumerate(LDA.components_):\n",
        "      print(f'Top {keywds} words for topic #{i}:')\n",
        "      print([count_vect.get_feature_names()[i] for i in topic.argsort()[-keywds:]])\n",
        "      print('\\n')\n",
        "  print('*'*50)\n",
        "  #put topic back to tweets\n",
        "  topic_values = LDA.transform(tweet_matrix)\n",
        "  df['HateTopic']=None\n",
        "  #putinto the main matrix\n",
        "  df.loc[df.index,'HateTopic']=topic_values.argmax(axis=1)\n",
        "  return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BCOhlew2jqK"
      },
      "source": [
        "print(len(T['compound']))\n",
        "print(sum(T['compound']>0)/len(T['compound']),sum(T['compound']<0)/len(T['compound']),sum(T['compound'].isnull())/len(T['compound']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4O_mByg2jqL"
      },
      "source": [
        "import scipy.stats as stats\n",
        "from itertools import combinations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_C_Sffp2jqL"
      },
      "source": [
        "not_selected_direction=['PerpetualProcesses','BiologicalProcesses','CognetiveProcess','InformalSpeech','Relativity','PersonalPronouns']\n",
        "selected_direction=['AffectWords','Customized','CoreDrivesandNeeds','PersonalConcerns','TimeOrientation']\n",
        "other=['SummaryVariable','Customized']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ5pubnv2jqL"
      },
      "source": [
        "### Study of Selected Country "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMTRrikItZaj"
      },
      "source": [
        "T_COUN=T[T['CountryCode'].isin(selected_country)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbO7pW9l2jqL"
      },
      "source": [
        "#all of the data is not normally distributed\n",
        "'''for LIWCcol in ['PersonalPronouns','AffectWords','CognetiveProcess','PerpetualProcesses','BiologicalProcesses','CoreDrivesandNeeds','TimeOrientation','Relativity','PersonalConcerns','InformalSpeech','Customized','SummaryVariable']:\n",
        "    for i in eval(LIWCcol):\n",
        "        for coun in selected_country:\n",
        "            print(LIWCcol,'shapiro',coun,stats.shapiro(T_COUN[T_COUN['CountryCode']==coun][i]))'''\n",
        "#Basically everything is normally distributed except summary attributes\n",
        "\n",
        "#create a stats table\n",
        "OneFrame=[]\n",
        "for LIWCcol in selected_direction:\n",
        "    for i in eval(LIWCcol):\n",
        "        CounCom=combinations(selected_country,2)\n",
        "        for couns in CounCom:\n",
        "            st=stats.ttest_ind(T_COUN[T_COUN['CountryCode']==couns[0]][i].dropna(),T_COUN[T_COUN['CountryCode']==couns[1]][i].dropna(),equal_var=False)\n",
        "            OneRow=[LIWCcol,i,couns[0],couns[1],st[0],st[1]]\n",
        "            OneFrame.append(OneRow)\n",
        "            #print(i,':',couns,stats.ranksums(T_COUN[T_COUN['CountryCode']==couns[0]][i],T_COUN[T_COUN['CountryCode']==couns[1]][i])[1])\n",
        "stats_table=pd.DataFrame(OneFrame,columns=['keyword1','keyword2','coun1','coun2','test_stats','p-value'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_rDK5EE2jqM"
      },
      "source": [
        "#stats_table.to_csv('User stats table based on countries.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ewSkTy32jqM"
      },
      "source": [
        "stats_table1=stats_table[stats_table['p-value']<0.05].sort_values('p-value')\n",
        "stats_table1['coun']=[frozenset([stats_table1.loc[i,'coun1'],stats_table1.loc[i,'coun2']]) for i in stats_table1.index]\n",
        "\n",
        "stats_table2=stats_table1.groupby('coun').agg({'keyword1':','.join,'keyword2':','.join})\n",
        "stats_table2['kw1list']=stats_table2['keyword1'].apply(lambda x: set(x.split(',')))\n",
        "stats_table2['kw2list']=stats_table2['keyword2'].apply(lambda x: set(x.split(',')))\n",
        "\n",
        "stats_table2['kw1len']=stats_table2['kw1list'].apply(lambda x:len(x))\n",
        "stats_table2['kw2len']=stats_table2['kw2list'].apply(lambda x:len(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW26PNsk2jqM"
      },
      "source": [
        "for i in selected_direction:\n",
        "    display(stats_table1[(stats_table1['coun'].apply(lambda x: 'Nigeria' in x)) & (stats_table1['keyword1']==i)].sort_values('keyword2'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP6-5ldg2jqM"
      },
      "source": [
        "#choose to include keywords for sdgs or not\n",
        "def addsdg(df):\n",
        "    newdf=df.copy(deep=True)\n",
        "    keyword_list=[['#poverty'],['#zerohunger'],['#globalhealth'],['#education'],['#genderequality'],['#water'],['#energy'],['#decentwork'],['#economicgrowth','#ideas'],    ['#socialjustice'],['#sustainablecities'],['#sparetosave'],['#climateaction'],['#ocean'],['#lifeonland'],['#justice','#peace']]\n",
        "    for i in range(16):\n",
        "        for ii in keyword_list[i]:\n",
        "            for iii in newdf[newdf['hashtags'].apply(lambda x: ii in x)].index:\n",
        "                newdf.loc[iii,'SDG'].append('SDG'+str(i+1))\n",
        "            #print(i+1,keyword_list[i],ii)\n",
        "    return newdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nt0Ajql2jqM"
      },
      "source": [
        "def ranksdg(df, add=False):\n",
        "    newdf=df\n",
        "    df_SDG=[]\n",
        "    for i in newdf['SDG'].to_list():\n",
        "        df_SDG.extend(i)\n",
        "    if add==True:\n",
        "        keyword_list=[['#poverty'],['#zerohunger'],['#globalhealth'],['#education'],['#genderequality'],['#water'],['#energy'],['#decentwork'],['#economicgrowth','#ideas'],    ['#socialjustice'],['#sustainablecities'],['#sparetosave'],['#climateaction'],['#ocean'],['#lifeonland'],['#justice','#peace']]\n",
        "        for i in range(16):\n",
        "            for ii in keyword_list[i]:\n",
        "                for iii in newdf[newdf['hashtags'].apply(lambda x: ii in x)].index:\n",
        "                    df_SDG.append('SDG'+str(i+1))\n",
        "    print(sorted(Counter(df_SDG).items(),key=lambda x:x[1], reverse=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF_980Nk2jqM"
      },
      "source": [
        "#analysis of country\n",
        "T_happy_Indian=T[(T['CountryCode']=='India') & (T['compound'] >0.9)]\n",
        "T_sad_Australia=T[(T['CountryCode']=='Australia') & (T['compound'] <-0.5)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxjbRqrH2jqN"
      },
      "source": [
        "T_Nigeria=T[(T['CountryCode']=='Nigeria')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY-cKrNd2jqN"
      },
      "source": [
        "T_US=T[(T['CountryCode']=='United States')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-4hfn8Y2jqN"
      },
      "source": [
        "print(*T_US[T_US['you']>10]['extended_tweet'].sample(3).tolist(),sep='\\n'*3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv_b592U2jqN"
      },
      "source": [
        "for LIWCcol in selected_direction:\n",
        "    colneeded=['CountryCode','id']\n",
        "    colneeded.extend(eval(LIWCcol))\n",
        "    aggdict={i:['mean','count'] for i in eval(LIWCcol)}\n",
        "    aggdict['id']='count'\n",
        "    DS=T_COUN[colneeded].groupby('CountryCode').agg(aggdict)\n",
        "    display(DS)\n",
        "    #DS.to_csv(LIWCcol+'median User.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4iWs4L62jqN"
      },
      "source": [
        "### LIWC analysis on developed and developing countries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chxKGlaZ2jqO"
      },
      "source": [
        "T_developed_developing=T[(T['CountryCode'].isin(developed_country)) | (T['CountryCode'].isin(developing_country))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ_2joW-2jqO"
      },
      "source": [
        "T_developed_developing['CountryType']=''\n",
        "T_developed_developing.loc[T['CountryCode'].isin(developed_country),'CountryType']='developed'\n",
        "T_developed_developing.loc[T['CountryCode'].isin(developing_country),'CountryType']='developing'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbTHhkR_2jqO"
      },
      "source": [
        "T_UK_sdg=PrintTopic(T[(T['CountryCode']=='Nigeria') & (T_developed_developing['extended_tweet'].str.contains('sdg2'))], topicnum=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUqOstOS2jqO"
      },
      "source": [
        "print(T_UK_sdg.shape)\n",
        "for i in range(2):\n",
        "    print(*T_UK_sdg[T_UK_sdg['HateTopic']==i]['extended_tweet'].drop_duplicates().sample(3).tolist(),sep='\\n'*3)\n",
        "    print('*'*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnzxPZ5s2jqO"
      },
      "source": [
        "def Percentage(x):\n",
        "        return round(len(x.dropna())/len(x),4)*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McbIOs7H2jqP"
      },
      "source": [
        "def Mean(x):\n",
        "    return round(x.mean(),2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSpY4QAO2jqP"
      },
      "source": [
        "def GenMeanPercen(df,selected_direction,needcol):\n",
        "    DS=[]\n",
        "    for LIWCcol in selected_direction:\n",
        "        colneeded=[needcol,'id']\n",
        "        colneeded.extend(eval(LIWCcol))\n",
        "        if LIWCcol=='AffectWords':\n",
        "            aggdict={i:[Mean,Percentage] for i in eval(LIWCcol)}\n",
        "        else:\n",
        "            aggdict={i:[Mean] for i in eval(LIWCcol)}\n",
        "        DS_temp=df[colneeded].groupby(needcol).agg(aggdict)\n",
        "        DS_ls=[]\n",
        "        for DS_col in DS_temp.columns:\n",
        "            #print((LIWCcol,)+DS_col)\n",
        "            DS_ls.append((LIWCcol,)+DS_col)\n",
        "        DS_temp.columns=pd.MultiIndex.from_tuples(DS_ls)\n",
        "        DS.append(DS_temp.T)\n",
        "    DS_verticle=pd.concat(DS)\n",
        "    '''DS_verticle['developed']=DS_verticle['developed'].round(2)\n",
        "    DS_verticle['developing']=DS_verticle['developing'].round(2)\n",
        "    len_developed=sum(T_developed_developing['CountryType']=='developed')\n",
        "    len_developing=sum(T_developed_developing['CountryType']=='developing')\n",
        "    for i in DS_verticle.index:\n",
        "        if 'count' in i:\n",
        "            print(round(DS_verticle.loc[i,'developed']/len_developed*100,4),'developed')\n",
        "            print(round(DS_verticle.loc[i,'developing']/len_developing*100,4),'developing')\n",
        "            DS_verticle.loc[i,'developing']=round(DS_verticle.loc[i,'developing']/len_developing*100,4)\n",
        "            DS_verticle.loc[i,'developed']=round(DS_verticle.loc[i,'developed']/len_developed*100,4)'''\n",
        "\n",
        "    return DS_verticle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLsL8JZP2jqP"
      },
      "source": [
        "C_combine=GenMeanPercen(T_developed_developing,selected_direction,'CountryType').join(GenMeanPercen(T_COUN,selected_direction,'CountryCode'))\n",
        "C_combine=C_combine[['developed', 'developing', 'Australia','United Kingdom', 'United States', 'India', 'Nigeria']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2J9X6812jqP"
      },
      "source": [
        "C_combine.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9Zt8z5B2jqP"
      },
      "source": [
        "C_combine.loc[('AffectWords','sad','Mean')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC92T7xe2jqQ"
      },
      "source": [
        "#C_combine.to_csv('all analysis developing and developed percentage.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byeatWDI2jqQ"
      },
      "source": [
        "#all of the data is not normally distributed\n",
        "'''for LIWCcol in ['AffectWords','CognetiveProcess','PerpetualProcesses','BiologicalProcesses','CoreDrivesandNeeds','TimeOrientation','Relativity','PersonalConcerns','InformalSpeech','Customized','SummaryVariable']:\n",
        "    for i in eval(LIWCcol):\n",
        "        for coun in ['developed','developing']:\n",
        "            print(LIWCcol,'shapiro',coun,stats.shapiro(T_developed_developing[T_developed_developing['CountryType']==coun][i]))'''\n",
        "#create a stats table\n",
        "OneFrame=[]\n",
        "for LIWCcol in selected_direction:\n",
        "    for i in eval(LIWCcol):\n",
        "        CounCom=combinations(['developed','developing'],2)\n",
        "        for couns in CounCom:\n",
        "            st=stats.ttest_ind(T_developed_developing[T_developed_developing['CountryType']==couns[0]][i].dropna(),T_developed_developing[T_developed_developing['CountryType']==couns[1]][i].dropna(),equal_var=False)\n",
        "            OneRow=[LIWCcol,i,couns[0],couns[1],st[0],st[1]]\n",
        "            OneFrame.append(OneRow)\n",
        "            #print(i,':',couns,stats.ranksums(T_COUN[T_COUN['CountryCode']==couns[0]][i],T_COUN[T_COUN['CountryCode']==couns[1]][i])[1])\n",
        "stats_table=pd.DataFrame(OneFrame,columns=['keyword1','keyword2','coun1','coun2','test_stats','p-value'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQcB4cZv2jqQ"
      },
      "source": [
        "stats_table1=stats_table[(stats_table['p-value']<0.01) & (stats_table['p-value']>=0.001)].sort_values('p-value')\n",
        "stats_table1['coun']=[frozenset([stats_table1.loc[i,'coun1'],stats_table1.loc[i,'coun2']]) for i in stats_table1.index]\n",
        "stats_table2=stats_table1.groupby('coun').agg({'keyword1':','.join,'keyword2':','.join})\n",
        "stats_table2['kw1list']=stats_table2['keyword1'].apply(lambda x: set(x.split(',')))\n",
        "stats_table2['kw2list']=stats_table2['keyword2'].apply(lambda x: set(x.split(',')))\n",
        "\n",
        "stats_table2['kw1len']=stats_table2['kw1list'].apply(lambda x:len(x))\n",
        "stats_table2['kw2len']=stats_table2['kw2list'].apply(lambda x:len(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPD909WK2jqQ"
      },
      "source": [
        "stats_table1.sort_values('keyword1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7HuA_nM2jqQ"
      },
      "source": [
        "#stats_table.to_csv('User stats table based on money.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN9XX6N02jqR"
      },
      "source": [
        "○ Affiliation\t-- McClelland-like\tdimensions\tincluding\treference\tto\tothers\n",
        "\n",
        "○ Achievement\t-- references\tto\tsuccess\tand\tfailure,\tachievement\tstriving\n",
        "\n",
        "○ Power\t-- references\trelevant\tto\tstatus,\tdominance,\tsocial\thierarchies\n",
        "\n",
        "○ Reward\tfocus\t-- references\tto\trewards,\tincentives,\tpositive\tgoals,\tapproach\n",
        "\n",
        "○ Risk\tfocus\t-- references\tto\tdangers,\tconcerns,\tthings\tto\tavoid\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fYb3u1e2jqR"
      },
      "source": [
        "#### miscellaneous"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAXLQQ3g2jqR"
      },
      "source": [
        "\n",
        "##negmo is complain and disapointment. \n",
        "#Need for power — driven by prestige, control, status, and influence over others\n",
        "#Need for achievement — driven by succeeding, accomplishing goals and overcoming challenges\n",
        "#Need for affiliation — motivated by close relationships with others\n",
        "#achievement is \n",
        "# power: powerful figures, law\n",
        "print(*T_developed_developing[(T_developed_developing['leisure']>10) & (T_developed_developing['extended_tweet'].str.contains('sdg'))]['extended_tweet'].sample(10).tolist(),sep='\\n'*3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur_uy6K82jqR"
      },
      "source": [
        "stats_table1.sort_values('keyword1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SNuASNQ2jqR"
      },
      "source": [
        "DS=[]\n",
        "for LIWCcol in selected_direction:\n",
        "    colneeded=['CountryCode','CountryType','id']\n",
        "    colneeded.extend(eval(LIWCcol))\n",
        "    aggdict={i:['mean','count'] for i in eval(LIWCcol)}\n",
        "    DS_temp=T_developed_developing[colneeded].groupby('CountryType').agg(aggdict)\n",
        "    DS_ls=[]\n",
        "    for DS_col in DS_temp.columns:\n",
        "        #print((LIWCcol,)+DS_col)\n",
        "        DS_ls.append((LIWCcol,)+DS_col)\n",
        "    DS_temp.columns=pd.MultiIndex.from_tuples(DS_ls)\n",
        "    DS.append(DS_temp.T)\n",
        "DS_verticle=pd.concat(DS)\n",
        "DS_verticle['developed']=DS_verticle['developed'].round(2)\n",
        "DS_verticle['developing']=DS_verticle['developing'].round(2)\n",
        "len_developed=sum(T_developed_developing['CountryType']=='developed')\n",
        "len_developing=sum(T_developed_developing['CountryType']=='developing')\n",
        "for i in DS_verticle.index:\n",
        "    if 'count' in i:\n",
        "        print(round(DS_verticle.loc[i,'developed']/len_developed*100,4),'developed')\n",
        "        print(round(DS_verticle.loc[i,'developing']/len_developing*100,4),'developing')\n",
        "        DS_verticle.loc[i,'developing']=round(DS_verticle.loc[i,'developing']/len_developing*100,4)\n",
        "        DS_verticle.loc[i,'developed']=round(DS_verticle.loc[i,'developed']/len_developed*100,4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EKLQpfQ2jqS"
      },
      "source": [
        "#DS_verticle.to_csv('all analysis developing and developed percentage.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhchRIy52jqS"
      },
      "source": [
        "for LIWCcol in selected_direction:\n",
        "    colneeded=['CountryCode','CountryType','id']\n",
        "    colneeded.extend(eval(LIWCcol))\n",
        "    aggdict={i:['mean','count'] for i in eval(LIWCcol)}\n",
        "    DS=T_developed_developing[colneeded].groupby('CountryType').agg(aggdict)\n",
        "    DS_ls=[]\n",
        "    for DS_col in DS.columns:\n",
        "        #print((LIWCcol,)+DS_col)\n",
        "        DS_ls.append((LIWCcol,)+DS_col)\n",
        "    DS.columns=pd.MultiIndex.from_tuples(DS_ls)\n",
        "    display(DS)\n",
        "    #DS.to_csv(LIWCcol+'median User money.csv') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0iBNWD5Hlbg"
      },
      "source": [
        "### Extreme Cases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjLQnByw2jqS"
      },
      "source": [
        "len(T[T['compound']>0.9]),len(T[T['compound']<-0.5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90V2AXAa2jqT"
      },
      "source": [
        "#### super positive ones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si2IWsUU2jqT"
      },
      "source": [
        "T_pos9=T[T['compound']>0.9]\n",
        "T_pos9=PrintTopic(T_pos9,topicnum=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifeIyX-02jqT"
      },
      "source": [
        "#T_pos9.to_csv('User positive larger 0.9.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIe3KnpN2jqT"
      },
      "source": [
        "#### super negative ones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPB8cYCt2jqT"
      },
      "source": [
        "print(*T[T['compound']<-0.5]['extended_tweet'].sample(5).tolist(),sep='\\n'+'*'*50+'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6V1FVwT2jqU"
      },
      "source": [
        "T_neg=T[T['compound']<-0.5]\n",
        "T_neg=PrintTopic(T_neg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Klc2p2D62jqU"
      },
      "source": [
        "#T_neg.to_csv('User negative smaller 0.5.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQbtQ_8X2jqU"
      },
      "source": [
        "#### select manually"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbHYRrpd2jqU"
      },
      "source": [
        "StrongAgainst=T[T['extended_tweet'].str.contains('nosdgs|noagenda21|nog30|agenda2030 propaganda|stop agenda21|lie of sdgs|lie of sustainabledevelopment|fuck agenda21|force agenda2030|sdgs is a lie|climateaction scam|lie of sustainabledevelopment',case=False)].drop_duplicates(subset='extended_tweet')#['extended_tweet']#.drop_duplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHg3_Orv2jqU"
      },
      "source": [
        "StrongAgainst['compound'].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRKVmqnT2jqV"
      },
      "source": [
        "print(*StrongAgainst['extended_tweet'].sample(3).tolist(),sep='\\n'+'-'*30+'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iivJjSQoL3EQ"
      },
      "source": [
        "StrongAgainst_HT=[]\n",
        "for i in StrongAgainst['hashtags'].tolist():\n",
        "  StrongAgainst_HT.extend(i)\n",
        "StrongAgainst_HT=Counter(StrongAgainst_HT)\n",
        "StrongAgainst_HT={i:k for i,k in sorted(StrongAgainst_HT.items(),key=lambda x:x[1],reverse=True)}\n",
        "#del StrongAgainst_HT['#sdg']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9glb4PUwFG8T"
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.bar(list(StrongAgainst_HT.keys())[:30],list(StrongAgainst_HT.values())[:30])\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "list(StrongAgainst_HT.keys())[:30]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwNhGGQFXcU7"
      },
      "source": [
        "topnum=4\n",
        "StrongAgainst=PrintTopic(StrongAgainst,topnum,20)\n",
        "\n",
        "for i in range(topnum):\n",
        "  print(*StrongAgainst[StrongAgainst['HateTopic']==i].sample()['extended_tweet'].tolist(),sep='\\n'+'-'*100+'\\n')\n",
        "  print('*'*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "zamKbHI-2jqV"
      },
      "source": [
        "print(*T[T['extended_tweet'].str.contains('firetrudeau')]['extended_tweet'].drop_duplicates().tolist(),sep='\\n'*3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipVX_Thr2jqW"
      },
      "source": [
        "#StrongAgainst.to_csv('User StrongAgainst 42.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKyov0-f2jqW"
      },
      "source": [
        "HT=[]\n",
        "for i in StrongAgainst[StrongAgainst['HateTopic']==1]['hashtags'].tolist():\n",
        "    HT.extend(i)\n",
        "sorted(Counter(HT).items(),key=lambda x:x[1],reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2tPE4Di2jqW"
      },
      "source": [
        "#### Plandemic related"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RR1Jv0cu2jqW"
      },
      "source": [
        "Plandemic=T[T['extended_tweet'].str.contains('nosdgs|noagenda21|nog30|agenda2030 propaganda|stop agenda21|lie of sdgs|lie of sustainabledevelopment|fuck agenda21|force agenda2030|sdgs is a lie|plandemic|lie of sustainabledevelopment|nogreatreset|nocia|noglobalism|noimf|noworldbank|nowef|vaxaware|nonewnormal|nonsa|novaccine|nomask|nocia|nofbi|noreset|nolockdown|scamdemic|nonwo',case=False)].drop_duplicates(subset='extended_tweet')#['extended_tweet']#.drop_duplicates()\n",
        "\n",
        "Plandemic=Plandemic.append(T[(T['extended_tweet'].str.contains('propaganda')) & (T['extended_tweet'].str.contains('agenda'))].drop_duplicates(subset='extended_tweet'))\n",
        "Plandemic=Plandemic.append(T[(T['extended_tweet'].str.contains('hypoc')) & (T['extended_tweet'].str.contains('agenda'))].drop_duplicates(subset='extended_tweet'))\n",
        "Plandemic=Plandemic.append(T[(T['extended_tweet'].str.contains('hypoc')) & (T['extended_tweet'].str.contains('sdg'))].drop_duplicates(subset='extended_tweet'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hgH8ME22jqW"
      },
      "source": [
        "Plandemic=Plandemic.drop_duplicates(subset='extended_tweet')\n",
        "Plandemic['old_tweet']=Plandemic['extended_tweet']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "QT3X2fKv2jqX"
      },
      "source": [
        "print(*T[(T['extended_tweet'].str.contains('nonwo')) & (T['extended_tweet'].str.contains(''))]['extended_tweet'].drop_duplicates().tolist(),sep='\\n'*3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "N8wbWNo-2jqX"
      },
      "source": [
        "for i in Plandemic.index:\n",
        "    tweet=Plandemic.loc[i,'extended_tweet']\n",
        "    for ii in Plandemic.loc[i,'hashtags']:\n",
        "        tweet=tweet.replace(ii[1:],'')\n",
        "    Plandemic.loc[i,'extended_tweet']=tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_4PcsVd2jqX"
      },
      "source": [
        "l=['scamdemic', 'people', 'refuse', 'plandemic', 'propaganda', 'agenda21', 'covid19', 'lie', 'agenda2030', 'sdgs']\n",
        "l.reverse()\n",
        "print(', '.join(l))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKuju2rR2jqX"
      },
      "source": [
        "topnum=2\n",
        "Plandemic=PrintTopic(Plandemic,topnum,10)\n",
        "\n",
        "for i in range(topnum):\n",
        "  print(*Plandemic[Plandemic['HateTopic']==i].sample(2)['old_tweet'].tolist(),sep='\\n'+'-'*100+'\\n')\n",
        "  print('*'*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ht0ee6c2jqY"
      },
      "source": [
        "P_topic_0,P_topic_1=[],[]\n",
        "for i in Plandemic[Plandemic['HateTopic']==0]['hashtags'].tolist():\n",
        "    P_topic_0.extend(i)\n",
        "for i in Plandemic[Plandemic['HateTopic']==1]['hashtags'].tolist():\n",
        "    P_topic_1.extend(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGBcLVxr2jqY"
      },
      "source": [
        "print(sorted(Counter(P_topic_0).items(),key=lambda x:x[1],reverse=True)[:15])\n",
        "print('\\n'*3)\n",
        "print(sorted(Counter(P_topic_1).items(),key=lambda x:x[1],reverse=True)[:15])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkMIO5dt2jqY"
      },
      "source": [
        "#### Network plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pylk3_42jqY"
      },
      "source": [
        "AgainstHT=[]\n",
        "for i in StrongAgainst['hashtags'].tolist():\n",
        "    AgainstHT.extend(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w59TK9vI2jqZ"
      },
      "source": [
        "import networkx as nx\n",
        "GC = nx.Graph()\n",
        "GC.add_nodes_from(set(AgainstHT))\n",
        "for hash_list in StrongAgainst.sort_values('created_at')['hashtags']:\n",
        "    GC.add_edges_from(combinations(hash_list,2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7li1OZa2jqZ"
      },
      "source": [
        "from operator import itemgetter\n",
        "plt.figure(figsize=(20,20))\n",
        "# find node with largest degree\n",
        "node_and_degree = GC.degree()\n",
        "(largest_hub, degree) = sorted(node_and_degree, key=itemgetter(1))[-1]\n",
        "\n",
        "# Create ego graph of main hub\n",
        "hub_ego = nx.ego_graph(GC, largest_hub)\n",
        "\n",
        "# Draw graph\n",
        "pos = nx.spring_layout(hub_ego)\n",
        "nx.draw(hub_ego, pos, node_color=\"b\", node_size=50, with_labels=True,font_weight='bold')\n",
        "\n",
        "# Draw ego as large and red\n",
        "\n",
        "options = {\"node_size\": 300, \"node_color\": \"r\"}\n",
        "nx.draw_networkx_nodes(hub_ego, pos, nodelist=[largest_hub], **options)\n",
        "#plt.savefig('User StrongAgainst Network.png',dpi=300)\n",
        "#plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR-1RlpG2jqZ"
      },
      "source": [
        "#nx.write_graphml(GC, '42 StrongAgainst Network.graphml')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}